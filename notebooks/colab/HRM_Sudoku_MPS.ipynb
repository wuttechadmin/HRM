{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2200a93",
   "metadata": {},
   "source": [
    "# 📚 How to Run This Notebook\n",
    "\n",
    "This notebook provides a step-by-step workflow for training and testing a Transformer model to solve Sudoku puzzles. Here's how to use it:\n",
    "\n",
    "## 🚀 Quick Start (Essential Cells)\n",
    "\n",
    "1. **Environment Check** (Cell with imports and device detection)\n",
    "   - Verifies Python, PyTorch, and MPS (Apple Silicon acceleration) availability\n",
    "   - Confirms dataset paths and structure\n",
    "   - Look for cell containing: `print(f\"Python version: {sys.version}\")`\n",
    "\n",
    "2. **Model & Dataset Setup** (Cell with class definitions)\n",
    "   - Loads required classes: `HRMSudokuDataset` and `SudokuTransformer`\n",
    "   - Defines utility functions for Sudoku validation and visualization\n",
    "   - Look for cell containing: `class HRMSudokuDataset(Dataset):`\n",
    "\n",
    "3. **Dataset Inspection** (Cell examining the data files)\n",
    "   - Examines dataset content and structure\n",
    "   - Verifies data integrity and consistency\n",
    "   - Look for cell containing: `train_files = sorted(train_path.glob(\"*.npy\"))`\n",
    "\n",
    "4. **Quick Verification Test** (Cell with mini-model testing)\n",
    "   - Runs a simple test to verify all components are working\n",
    "   - Creates a small dataset and model to confirm functionality\n",
    "   - Look for cell containing: `mini_config = {`\n",
    "\n",
    "5. **Basic Functionality Test** (Cell with model testing functions)\n",
    "   - Performs a more thorough test of model and dataset\n",
    "   - Tests forward pass and solution validity\n",
    "   - Look for cell containing: `def test_model_on_sample(model, dataset, sample_idx=0):`\n",
    "\n",
    "6. **Mini Training Loop** (Cell with training loop implementation)\n",
    "   - Runs a short training loop with small dataset\n",
    "   - Visualizes training loss and tests on a validation sample\n",
    "   - Look for cell containing: `train_losses = []`\n",
    "\n",
    "## 📊 Additional Features (Optional Cells)\n",
    "\n",
    "7. **Model Diagnostics** (Cell with visualization functions)\n",
    "   - Visualizes model errors with heatmaps\n",
    "   - Analyzes which positions are most difficult to predict\n",
    "   - Look for cell containing: `def plot_error_heatmap(` or `def analyze_position_difficulty(`\n",
    "\n",
    "8. **Custom Puzzle Test** (Cell with custom puzzle input)\n",
    "   - Tests the model on a pre-defined or custom Sudoku puzzle\n",
    "   - Visualizes and validates the model's solution\n",
    "   - Look for cell containing: `custom_puzzle = \"\"\"` or `run_puzzle_solver`\n",
    "\n",
    "## 🔍 How to Execute\n",
    "\n",
    "1. **Run cells in sequence** (from top to bottom)\n",
    "2. **Wait for each cell to complete** before moving to the next one\n",
    "3. **Check outputs** to verify proper execution\n",
    "4. **For quick experimentation**, just run the Environment Check, Model & Dataset Setup, Dataset Inspection, and Quick Verification Test cells\n",
    "\n",
    "> **Note**: The notebook is designed for incremental testing - each component can be tested independently once the core Environment Check and Model & Dataset Setup cells are executed.\n",
    "\n",
    "> **Important**: Don't rely on cell numbers as they may differ between notebook interfaces. Instead, look for the cell titles and code content described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f853d3",
   "metadata": {},
   "source": [
    "# 📝 Summary of Incremental Improvements\n",
    "\n",
    "This notebook now provides a complete, working environment for training and evaluating Sudoku-solving models with PyTorch on MacOS (MPS). The key components are:\n",
    "\n",
    "## 📊 Workflow Diagram\n",
    "\n",
    "```\n",
    "┌─────────────────────┐\n",
    "│  Environment Check  │◄───────┐\n",
    "└──────────┬──────────┘        │\n",
    "           ▼                   │\n",
    "┌─────────────────────┐        │\n",
    "│ Model & Dataset     │        │ Core Setup\n",
    "│ Setup               │        │ (Required First)\n",
    "└──────────┬──────────┘        │\n",
    "           ▼                   │\n",
    "┌─────────────────────┐        │\n",
    "│ Dataset Inspection  │────────┘\n",
    "└──────────┬──────────┘\n",
    "           │\n",
    "           ├─────────────┬─────────────┬─────────────┐\n",
    "           ▼             ▼             ▼             ▼\n",
    "┌─────────────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐\n",
    "│     Quick       │ │   Mini    │ │  Model    │ │  Custom   │\n",
    "│  Verification   │ │ Training  │ │ Diagnostics│ │  Puzzle   │\n",
    "│     Test        │ │   Loop    │ │           │ │   Test    │\n",
    "└─────────────────┘ └───────────┘ └───────────┘ └───────────┘\n",
    "           ▲                                           ▲\n",
    "           │                                           │\n",
    "           └───────────────────────────────────────────┘\n",
    "                    Can run independently once\n",
    "                      core setup is complete\n",
    "```\n",
    "\n",
    "## ✅ Working Components\n",
    "1. **Environment Setup & Dataset Verification**\n",
    "   - Correctly identifies MPS device when available\n",
    "   - Validates dataset integrity and checks for clue-solution consistency\n",
    "\n",
    "2. **Core Model Architecture**\n",
    "   - Simple Transformer model with positional encoding\n",
    "   - Specialized for Sudoku's 9x9 structure with digit constraints\n",
    "   - Properly handles input clues vs. cells to be predicted\n",
    "\n",
    "3. **Training & Evaluation**\n",
    "   - Mini-training loop with loss visualization\n",
    "   - Model diagnostics with error heatmaps and position analysis\n",
    "   - Custom puzzle testing with solution validation\n",
    "\n",
    "## 🔄 Future Improvements\n",
    "1. **Model Architecture Enhancements**\n",
    "   - Add specialized layers for Sudoku constraints (row/column/box checks)\n",
    "   - Implement attention mechanisms focused on Sudoku rule relationships\n",
    "   - Experiment with different positional encodings optimized for grid structures\n",
    "\n",
    "2. **Training Strategies**\n",
    "   - Progressive difficulty curriculum learning\n",
    "   - Data augmentation through puzzle rotation and transposition\n",
    "   - Specialized loss functions that incorporate Sudoku validity\n",
    "\n",
    "3. **Evaluation Metrics**\n",
    "   - Track solution validity rates and rule violations\n",
    "   - Analyze performance by puzzle difficulty levels\n",
    "   - Compare with traditional algorithmic solvers\n",
    "\n",
    "> **Note**: Look for descriptive cell titles and code content rather than cell numbers, as they may differ between notebook interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if required packages are installed, and install if missing\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "required_packages = ['pandas', 'matplotlib', 'ipywidgets', 'torch', 'numpy', 'tqdm']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    if importlib.util.find_spec(package) is None:\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"Installing missing packages: {', '.join(missing_packages)}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
    "    print(\"✅ Installation complete. You may need to restart the kernel.\")\n",
    "\n",
    "# Now import all required packages\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Setup paths\n",
    "ROOT_DIR = Path(os.getcwd())\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"sudoku-extreme-1k-aug-1000\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "MODEL_DIR = ROOT_DIR / \"models\"\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check for CUDA/MPS (Apple Silicon)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = \"CUDA\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    device_name = \"MPS (Apple Silicon)\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device_name}\")\n",
    "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
    "\n",
    "# Function for interactive plotting\n",
    "def create_interactive_plot():\n",
    "    \"\"\"Create interactive plot with subplots for tracking metrics\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Create line objects for the plots\n",
    "    lines = {\n",
    "        'loss': axs[0].plot([], [], 'b-', label='Train Loss')[0],\n",
    "        'acc': axs[1].plot([], [], 'g-', label='Cell Accuracy')[0],\n",
    "        'valid': axs[2].plot([], [], 'r-', label='Valid Solutions')[0],\n",
    "        'exact': axs[2].plot([], [], 'c-', label='Exact Matches')[0]\n",
    "    }\n",
    "    \n",
    "    # Set up the plots\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].set_xlabel('Iteration')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    axs[1].set_title('Cell Accuracy')\n",
    "    axs[1].set_xlabel('Iteration')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].set_ylim(0, 1)\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    axs[2].set_title('Solution Quality')\n",
    "    axs[2].set_xlabel('Iteration')\n",
    "    axs[2].set_ylabel('Rate')\n",
    "    axs[2].set_ylim(0, 1)\n",
    "    axs[2].grid(True)\n",
    "    axs[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axs, lines\n",
    "\n",
    "# Function to update the interactive plot\n",
    "def update_plot(fig, lines, history):\n",
    "    \"\"\"Update the interactive plot with new data\"\"\"\n",
    "    if 'train_loss' in history and len(history['train_loss']) > 0:\n",
    "        x = list(range(len(history['train_loss'])))\n",
    "        lines['loss'].set_data(x, history['train_loss'])\n",
    "        lines['loss'].axes.relim()\n",
    "        lines['loss'].axes.autoscale_view()\n",
    "    \n",
    "    if 'val_cell_accuracy' in history and len(history['val_cell_accuracy']) > 0:\n",
    "        x = list(range(len(history['val_cell_accuracy'])))\n",
    "        lines['acc'].set_data(x, history['val_cell_accuracy'])\n",
    "        lines['acc'].axes.relim()\n",
    "        lines['acc'].axes.autoscale_view()\n",
    "    \n",
    "    if 'val_valid_solutions' in history and len(history['val_valid_solutions']) > 0:\n",
    "        x = list(range(len(history['val_valid_solutions'])))\n",
    "        lines['valid'].set_data(x, history['val_valid_solutions'])\n",
    "        lines['valid'].axes.relim()\n",
    "        lines['valid'].axes.autoscale_view()\n",
    "    \n",
    "    if 'val_exact_match' in history and len(history['val_exact_match']) > 0:\n",
    "        x = list(range(len(history['val_exact_match'])))\n",
    "        lines['exact'].set_data(x, history['val_exact_match'])\n",
    "        lines['exact'].axes.relim()\n",
    "        lines['exact'].axes.autoscale_view()\n",
    "    \n",
    "    # Redraw the figure\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    display(fig)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Function to create a cell-level error heatmap\n",
    "def plot_error_heatmap(model, dataset_sample, device):\n",
    "    \"\"\"Create a heatmap showing where the model makes errors in the Sudoku grid\"\"\"\n",
    "    input_grid = dataset_sample['input_ids'].to(device)\n",
    "    target_grid = dataset_sample['target'].cpu().numpy().reshape(9, 9)\n",
    "    \n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_grid.unsqueeze(0))\n",
    "        # Ensure we only consider valid Sudoku digits (0-9)\n",
    "        logits = logits[:, :, :10]\n",
    "        pred = logits.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Ensure clues are preserved\n",
    "        non_zero_mask = dataset_sample['input_ids'].numpy() > 0\n",
    "        pred[non_zero_mask] = dataset_sample['input_ids'].numpy()[non_zero_mask]\n",
    "        \n",
    "    pred_grid = pred.reshape(9, 9)\n",
    "    \n",
    "    # Create error mask (1 for error, 0 for correct)\n",
    "    error_mask = (pred_grid != target_grid).astype(int)\n",
    "    \n",
    "    # Create a mask for input clues (1 for clues, 0 for filled cells)\n",
    "    clue_mask = dataset_sample['input_ids'].numpy().reshape(9, 9) > 0\n",
    "    \n",
    "    # Combine into a single visualization grid\n",
    "    # 0: Correct prediction\n",
    "    # 1: Error\n",
    "    # 2: Original clue\n",
    "    vis_grid = error_mask.copy()\n",
    "    vis_grid[clue_mask] = 2\n",
    "    \n",
    "    # Create a custom colormap (green for correct, red for errors, blue for clues)\n",
    "    cmap = ListedColormap(['lightgreen', 'tomato', 'lightskyblue'])\n",
    "    \n",
    "    # Create figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    im = ax1.imshow(vis_grid, cmap=cmap, vmin=0, vmax=2)\n",
    "    ax1.set_title('Error Analysis')\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(10):\n",
    "        lw = 2 if i % 3 == 0 else 0.5\n",
    "        ax1.axhline(i - 0.5, color='black', linewidth=lw)\n",
    "        ax1.axvline(i - 0.5, color='black', linewidth=lw)\n",
    "    \n",
    "    # Create legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightgreen', label='Correct'),\n",
    "        Patch(facecolor='tomato', label='Error'),\n",
    "        Patch(facecolor='lightskyblue', label='Clue')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "    \n",
    "    # Add comparison grid showing actual digits\n",
    "    # Create a grid with both predicted and target values\n",
    "    comparison_grid = np.zeros((9, 9), dtype=object)\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if clue_mask[i, j]:\n",
    "                # Clue cell - show in blue\n",
    "                comparison_grid[i, j] = f\"${pred_grid[i, j]}$\"\n",
    "            elif pred_grid[i, j] == target_grid[i, j]:\n",
    "                # Correct prediction - show in green\n",
    "                comparison_grid[i, j] = f\"${pred_grid[i, j]}$\"\n",
    "            else:\n",
    "                # Error - show prediction/target in red\n",
    "                comparison_grid[i, j] = f\"${pred_grid[i, j]}\\\\neq{target_grid[i, j]}$\"\n",
    "    \n",
    "    # Create a table for the second subplot\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "    table = ax2.table(cellText=comparison_grid, loc='center', cellLoc='center')\n",
    "    \n",
    "    # Style the table\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    # Color the cells\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            cell = table[(i, j)]\n",
    "            if clue_mask[i, j]:\n",
    "                cell.set_facecolor('lightskyblue')\n",
    "            elif pred_grid[i, j] == target_grid[i, j]:\n",
    "                cell.set_facecolor('lightgreen')\n",
    "            else:\n",
    "                cell.set_facecolor('tomato')\n",
    "    \n",
    "    # Add grid lines for 3x3 boxes\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            rect = plt.Rectangle((j*3-0.5, i*3-0.5), 3, 3, fill=False, color='black', linewidth=2)\n",
    "            ax1.add_patch(rect)\n",
    "    \n",
    "    ax2.set_title('Value Comparison (Pred ≠ Target)')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Function to analyze model performance by position\n",
    "def analyze_position_difficulty(model, dataset, device, num_samples=50):\n",
    "    \"\"\"Analyze which positions in the Sudoku grid are most difficult for the model\"\"\"\n",
    "    error_counts = np.zeros((9, 9))\n",
    "    total_counts = np.zeros((9, 9))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(len(dataset), num_samples)):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].to(device)\n",
    "            target = sample['target'].cpu().numpy()\n",
    "            \n",
    "            # Skip samples with too many clues (not interesting for analysis)\n",
    "            clue_count = (input_ids.cpu().numpy() > 0).sum()\n",
    "            if clue_count > 40:  # Skip if more than 40 clues\n",
    "                continue\n",
    "                \n",
    "            # Get prediction\n",
    "            logits = model(input_ids.unsqueeze(0))\n",
    "            logits = logits[:, :, :10]  # Only consider valid digits\n",
    "            pred = logits.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Ensure clues are preserved\n",
    "            non_zero_mask = sample['input_ids'].numpy() > 0\n",
    "            pred[non_zero_mask] = sample['input_ids'].numpy()[non_zero_mask]\n",
    "            \n",
    "            # Count errors by position (only for cells model needed to fill)\n",
    "            zero_mask = sample['input_ids'].numpy() == 0\n",
    "            error_mask = (pred != target) & zero_mask\n",
    "            \n",
    "            # Update counts for positions that needed filling\n",
    "            for pos in np.where(zero_mask)[0]:\n",
    "                row, col = pos // 9, pos % 9\n",
    "                total_counts[row, col] += 1\n",
    "                if error_mask[pos]:\n",
    "                    error_counts[row, col] += 1\n",
    "    \n",
    "    # Calculate error rates\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        error_rates = np.where(total_counts > 0, error_counts / total_counts, 0)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(error_rates, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel('Error Rate', rotation=-90, va=\"bottom\")\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(10):\n",
    "        lw = 2 if i % 3 == 0 else 0.5\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=lw)\n",
    "        ax.axvline(i - 0.5, color='black', linewidth=lw)\n",
    "    \n",
    "    # Add labels\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if total_counts[i, j] > 0:\n",
    "                text = f\"{error_rates[i, j]:.2f}\\n({int(error_counts[i, j])}/{int(total_counts[i, j])})\"\n",
    "                ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\" if error_rates[i, j] < 0.5 else \"white\", fontsize=8)\n",
    "    \n",
    "    ax.set_title('Error Rates by Position')\n",
    "    plt.tight_layout()\n",
    "    return fig, error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e121e",
   "metadata": {},
   "source": [
    "# 🚀 Enhanced Sudoku Model Training with Real-Time Monitoring\n",
    "\n",
    "This notebook has been improved with comprehensive training and monitoring features:\n",
    "\n",
    "## 1. Interactive Training Visualization\n",
    "- **Real-Time Metrics**: Watch training loss, accuracy, and solution rates update live\n",
    "- **Cell-Level Error Analysis**: Visualize which positions cause the most trouble\n",
    "- **Position Difficulty Heatmaps**: Identify pattern-specific learning issues\n",
    "\n",
    "## 2. Training Strategies\n",
    "- **Progressive Complexity Training**: Start with simpler puzzles, then increase difficulty\n",
    "- **Checkpoint Management**: Save and resume training from the best models\n",
    "- **Flexible Model Configuration**: Easily adjust model size and training parameters\n",
    "\n",
    "## 3. Enhanced Validation\n",
    "- **Solution Verification**: Explicitly check if solutions are valid Sudoku puzzles\n",
    "- **Detailed Error Analysis**: Analyze error patterns and distribution\n",
    "- **Metrics By Difficulty**: Track performance across puzzles of varying complexity\n",
    "\n",
    "## 4. Model Debugging\n",
    "- **Cell-by-Cell Comparison**: Compare model outputs with expected solutions\n",
    "- **Error Visualization**: Generate heatmaps to understand error patterns\n",
    "- **Cross-Stage Analysis**: Track improvement as difficulty increases\n",
    "\n",
    "These improvements allow for faster experimentation cycles, better understanding of model behavior, and improved solution quality. The notebook automatically tracks key metrics needed to diagnose and fix issues with Sudoku puzzle solving.\n",
    "\n",
    "## Training Configuration Improvements\n",
    "\n",
    "### 1. Architecture Enhancements\n",
    "- **Larger Model Size**: Increased `hidden_size` from 96 to 192 for better representational capacity\n",
    "  - *Justification*: Sudoku requires understanding complex spatial relationships between numbers in rows, columns, and boxes. A larger hidden size allows the model to represent these relationships more effectively.\n",
    "  \n",
    "- **Deeper Network**: Increased `num_layers` from 3 to 6 for more complex reasoning capabilities\n",
    "  - *Justification*: Solving Sudoku often requires multi-step logical reasoning. Additional layers help the model chain together these logical steps.\n",
    "  \n",
    "- **More Attention Heads**: Increased `num_heads` from 4 to 8 for better pattern recognition\n",
    "  - *Justification*: Each attention head can specialize in different types of patterns (rows, columns, boxes, etc.). More heads allow the model to simultaneously attend to different Sudoku constraints.\n",
    "\n",
    "### 2. Training Process Optimizations\n",
    "- **Learning Rate Schedule**: Implemented cosine learning rate schedule with warmup for better convergence\n",
    "  - *Justification*: Cosine schedules gradually reduce learning rate, allowing fine-grained optimization in later training stages while avoiding local minima. The warmup period helps stabilize early training.\n",
    "  \n",
    "- **Early Stopping**: Added patience-based early stopping to prevent overfitting\n",
    "  - *Justification*: Stops training when validation accuracy plateaus, preventing the model from memorizing training examples rather than learning general patterns.\n",
    "  \n",
    "- **Higher Batch Size**: Balanced batch size (64) for better gradient estimation and MPS utilization\n",
    "  - *Justification*: Larger batches provide more stable gradient estimates. We've chosen 64 as a balance between memory constraints on MPS and training stability.\n",
    "  \n",
    "- **Increased Regularization**: Higher weight decay (0.02) to prevent overfitting\n",
    "  - *Justification*: Sudoku has clear rules but limited patterns. Stronger regularization prevents the model from memorizing specific puzzles instead of learning the underlying logic.\n",
    "\n",
    "### 3. Validation Improvements\n",
    "- **Comprehensive Metrics**: Track exact matches, valid solutions, and cell-level accuracy\n",
    "  - *Justification*: Cell-level accuracy alone isn't sufficient for Sudoku. A single incorrect cell makes the entire puzzle invalid, so we track multiple metrics.\n",
    "  \n",
    "- **Traditional Solver Comparison**: Added a traditional backtracking Sudoku solver for baseline comparison\n",
    "  - *Justification*: Comparing against a traditional algorithm helps understand if the model is truly learning logical rules or just approximating patterns.\n",
    "\n",
    "### 4. Performance Analysis\n",
    "- **Cell-by-Cell Analysis**: Detailed comparison of model predictions vs. expected solutions\n",
    "  - *Justification*: Identifies specific patterns of errors, which helps understand what logical rules the model struggles with.\n",
    "  \n",
    "- **Training Progress Visualization**: Optional plotting of loss and accuracy curves\n",
    "  - *Justification*: Visual feedback on training progress helps identify issues like overfitting or poor convergence early.\n",
    "\n",
    "These changes should significantly improve the model's ability to learn logical reasoning patterns required for solving Sudoku puzzles, while maintaining compatibility with MPS acceleration on MacOS. The balance between model capacity and computational efficiency is optimized for Apple Silicon processors.\n",
    "\n",
    "# 🔄 Notebook Workflow\n",
    "\n",
    "```\n",
    "┌────────────────────┐    ┌────────────────────┐    ┌────────────────────┐\n",
    "│  1. Environment    │    │  2. Model &        │    │  3. Dataset        │\n",
    "│     Check          │───►│     Dataset Setup  │───►│     Inspection     │\n",
    "└────────────────────┘    └────────────────────┘    └──────────┬─────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "┌────────────────────┐    ┌────────────────────┐    ┌────────────────────┐\n",
    "│  6. Custom         │    │  5. Model          │    │  4. Quick          │\n",
    "│     Puzzle Test    │◄───│     Diagnostics    │◄───│     Verification   │\n",
    "└────────────┬───────┘    └────────────┬───────┘    └──────────┬─────────┘\n",
    "             │                         │                       │\n",
    "             └─────────────────┬───────┘                       │\n",
    "                               ▼                               ▼\n",
    "                      ┌────────────────────┐         ┌────────────────────┐\n",
    "                      │  Optional          │         │  7. Mini Training  │\n",
    "                      │  Advanced Features │◄────────│     Loop           │\n",
    "                      └────────────────────┘         └────────────────────┘\n",
    "```\n",
    "\n",
    "This notebook implements an incremental approach to building and testing a Sudoku solver:\n",
    "\n",
    "1. First, we set up the environment and verify the dataset\n",
    "2. Then we define the model architecture and dataset classes\n",
    "3. We inspect the dataset to ensure it's valid\n",
    "4. We test basic functionality (model creation, forward pass)\n",
    "5. We run a minimal training session to verify learning\n",
    "6. Finally, we can test the model on custom puzzles and analyze its performance\n",
    "\n",
    "Each component is designed to be tested independently, allowing for focused debugging and incremental improvements.\n",
    "\n",
    "> **Note**: Look for descriptive cell titles and code content rather than cell numbers, as they may differ between notebook interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a44041",
   "metadata": {},
   "source": [
    "# 🔍 Sudoku Model Debugging and Training\n",
    "\n",
    "This notebook has been updated to ensure proper training and valid outputs for Sudoku puzzles. The key changes include:\n",
    "\n",
    "1. **Vocabulary Size Restriction**: Fixed to use `vocab_size=10` consistently (digits 0-9 only)\n",
    "2. **Logit Slicing**: Added `output_logits[:, :, :10]` to ensure we only consider valid Sudoku digits\n",
    "3. **Model Training**: Added explicit training with early stopping for better performance\n",
    "4. **Validation Enhancement**: Added cell-by-cell comparison and detailed metrics\n",
    "5. **User Example Testing**: Added support for testing with the specific example provided by the user\n",
    "\n",
    "These changes should fix the issues with invalid digits (>9) appearing in model outputs and improve overall accuracy.\n",
    "\n",
    "> **Note**: Look for descriptive cell titles and code content rather than cell numbers, as they may differ between notebook interfaces.\n",
    "\n",
    "# 🔄 Notebook Workflow\n",
    "\n",
    "```\n",
    "┌────────────────────┐    ┌────────────────────┐    ┌────────────────────┐\n",
    "│  1. Environment    │    │  2. Model &        │    │  3. Dataset        │\n",
    "│     Check          │───►│     Dataset Setup  │───►│     Inspection     │\n",
    "│   [Cell #8]        │    │   [Cell #9]        │    │   [Cell #10]       │\n",
    "└────────────────────┘    └────────────────────┘    └──────────┬─────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "┌────────────────────┐    ┌────────────────────┐    ┌────────────────────┐\n",
    "│  6. Custom         │    │  5. Model          │    │  4. Quick          │\n",
    "│     Puzzle Test    │◄───│     Diagnostics    │◄───│     Verification   │\n",
    "│   [Cell #26]       │    │   [Cell #13]       │    │   [Cell #11]       │\n",
    "└────────────┬───────┘    └────────────┬───────┘    └──────────┬─────────┘\n",
    "             │                         │                       │\n",
    "             └─────────────────┬───────┘                       │\n",
    "                               ▼                               ▼\n",
    "                      ┌────────────────────┐         ┌────────────────────┐\n",
    "                      │  Advanced          │         │  7. Mini Training  │\n",
    "                      │  Features          │◄────────│   [Cell #12]       │\n",
    "                      └────────────────────┘         └────────────────────┘\n",
    "```\n",
    "\n",
    "This notebook implements an incremental approach to building and testing a Sudoku solver:\n",
    "\n",
    "1. First, we set up the environment and verify the dataset\n",
    "2. Then we define the model architecture and dataset classes\n",
    "3. We inspect the dataset to ensure it's valid\n",
    "4. We run quick verification tests to confirm everything works\n",
    "5. We run a minimal training session to verify learning\n",
    "6. Finally, we can test the model on custom puzzles and analyze its performance\n",
    "\n",
    "Each component is designed to be tested independently, allowing for focused debugging and incremental improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0700877",
   "metadata": {},
   "source": [
    "# HRM Sudoku Model - MacOS/MPS Version\n",
    "\n",
    "This notebook demonstrates the training and evaluation of a Hierarchical Relational Model (HRM) on Sudoku puzzles. This version is optimized for MacOS with MPS (Metal Performance Shaders) acceleration.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic device detection (MPS/CPU)\n",
    "- Strict input/solution validation\n",
    "- Dataset repair capabilities\n",
    "- Visualization of puzzles and solutions\n",
    "- Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90328d2",
   "metadata": {},
   "source": [
    "# 🧩 HRM Sudoku-Extreme 1k Demo\n",
    "**MacOS version with MPS backend**  \n",
    "Adapted from the Google Colab notebook for MacOS with Apple Silicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Check \n",
    "# This cell checks system compatibility, device availability, and dataset existence\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import platform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"System: {platform.system()} {platform.release()} {platform.machine()}\")\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")\n",
    "\n",
    "# Determine the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS (Metal Performance Shaders) for Apple Silicon\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU\")\n",
    "\n",
    "# Check working directory and data\n",
    "print(f\"\\nCurrent working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check for dataset\n",
    "data_path = Path(\"/Users/robertburkhall/Development/HRM/data/sudoku-extreme-1k-aug-1000\")\n",
    "if data_path.exists():\n",
    "    print(f\"Dataset found at: {data_path}\")\n",
    "    \n",
    "    # Check test and train directories\n",
    "    test_path = data_path / \"test\"\n",
    "    train_path = data_path / \"train\"\n",
    "    \n",
    "    if test_path.exists() and train_path.exists():\n",
    "        print(f\"✅ Test and train directories found\")\n",
    "        \n",
    "        # Check for data files\n",
    "        test_files = list(test_path.glob(\"*.npy\"))\n",
    "        train_files = list(train_path.glob(\"*.npy\"))\n",
    "        \n",
    "        print(f\"Test files found: {len(test_files)}\")\n",
    "        print(f\"Train files found: {len(train_files)}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing test or train directories\")\n",
    "else:\n",
    "    print(f\"❌ Dataset not found at: {data_path}\")\n",
    "    print(\"Please check that the data directory exists and is correctly named.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & Dataset Setup\n",
    "# This cell defines the core model and dataset classes for the Sudoku transformer\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete HRM Sudoku Demo - One Cell End-to-End\n",
    "Everything in one script: dataset loading, training, evaluation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['USE_FLASH_ATTN'] = 'false'\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "print(\"🎯 HRM Sudoku Complete Demo - MacOS Version\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm  # Use regular tqdm instead of tqdm.notebook\n",
    "\n",
    "# Dataset class for HRM Sudoku\n",
    "class HRMSudokuDataset(Dataset):\n",
    "    \"\"\"Dataset loader for HRM Sudoku data format\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, split='train', max_samples=None):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.vocab_size = 10  # Using 0-9 for Sudoku\n",
    "        \n",
    "        print(f\"\\n🔍 Loading HRM dataset from: {self.data_path / split}\")\n",
    "        \n",
    "        split_dir = self.data_path / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"❌ Directory {split_dir} not found\")\n",
    "            return\n",
    "            \n",
    "        # Try to directly load the numpy files we expect\n",
    "        inputs_file = split_dir / \"all__inputs.npy\"\n",
    "        labels_file = split_dir / \"all__labels.npy\"\n",
    "        \n",
    "        if inputs_file.exists() and labels_file.exists():\n",
    "            print(f\"✅ Found standard HRM format files\")\n",
    "            try:\n",
    "                inputs = np.load(inputs_file)\n",
    "                labels = np.load(labels_file)\n",
    "                \n",
    "                print(f\"📊 Loaded arrays - inputs: {inputs.shape}, labels: {labels.shape}\")\n",
    "                \n",
    "                if len(inputs) == len(labels):\n",
    "                    # Limit samples if max_samples is specified\n",
    "                    sample_count = len(inputs) if max_samples is None else min(len(inputs), max_samples)\n",
    "                    \n",
    "                    # Verify and add samples with validation\n",
    "                    valid_count = 0\n",
    "                    for i in range(sample_count):\n",
    "                        if self._add_validated_sample(inputs[i], labels[i]):\n",
    "                            valid_count += 1\n",
    "                    \n",
    "                    print(f\"✅ Added {valid_count} validated samples\")\n",
    "                    \n",
    "                    # Load metadata if available\n",
    "                    self._load_metadata(split_dir)\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error loading standard files: {e}\")\n",
    "        \n",
    "        print(f\"⚠️ No samples loaded from {split_dir}\")\n",
    "    \n",
    "    def _is_valid_sudoku(self, grid):\n",
    "        \"\"\"Check if 9x9 grid is valid Sudoku solution\"\"\"\n",
    "        # Check rows\n",
    "        for i in range(9):\n",
    "            row = grid[i, :]\n",
    "            row_no_zeros = row[row != 0]\n",
    "            if len(row_no_zeros) != len(set(row_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "        # Check columns\n",
    "        for i in range(9):\n",
    "            col = grid[:, i]\n",
    "            col_no_zeros = col[col != 0]\n",
    "            if len(col_no_zeros) != len(set(col_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "        # Check 3x3 boxes\n",
    "        for box_row in range(3):\n",
    "            for box_col in range(3):\n",
    "                box = grid[box_row*3:(box_row+1)*3, box_col*3:(box_col+1)*3].flatten()\n",
    "                box_no_zeros = box[box != 0]\n",
    "                if len(box_no_zeros) != len(set(box_no_zeros)):\n",
    "                    return False\n",
    "                    \n",
    "        return True\n",
    "    \n",
    "    def _add_validated_sample(self, input_data, target_data):\n",
    "        \"\"\"Add a sample with validation to ensure input/solution consistency\"\"\"\n",
    "        try:\n",
    "            input_array = np.array(input_data, dtype=np.int64)\n",
    "            target_array = np.array(target_data, dtype=np.int64)\n",
    "\n",
    "            # Cap values at 9 for Sudoku\n",
    "            input_array = np.clip(input_array, 0, 9)\n",
    "            target_array = np.clip(target_array, 0, 9)\n",
    "\n",
    "            if not (len(input_array) == 81 and len(target_array) == 81):\n",
    "                return False\n",
    "\n",
    "            if not (np.all(input_array >= 0) and np.all(input_array < self.vocab_size) and\n",
    "                   np.all(target_array >= 0) and np.all(target_array < self.vocab_size)):\n",
    "                return False\n",
    "\n",
    "            # CRITICAL: Ensure all non-zero input values match the target values\n",
    "            # This is essential for valid Sudoku puzzles\n",
    "            non_zero_mask = input_array > 0\n",
    "            if not np.all(input_array[non_zero_mask] == target_array[non_zero_mask]):\n",
    "                return False\n",
    "                \n",
    "            # Validate solution is a proper Sudoku grid\n",
    "            if not self._is_valid_sudoku(target_array.reshape(9, 9)):\n",
    "                return False\n",
    "\n",
    "            self.samples.append({\n",
    "                'input_ids': torch.tensor(input_array, dtype=torch.long),\n",
    "                'target': torch.tensor(target_array, dtype=torch.long)\n",
    "            })\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding sample: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _load_metadata(self, split_dir):\n",
    "        \"\"\"Load metadata from dataset.json\"\"\"\n",
    "        metadata_file = split_dir / \"dataset.json\"\n",
    "        if metadata_file.exists():\n",
    "            try:\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                print(f\"📊 Metadata: vocab_size={metadata.get('vocab_size', 10)}\")\n",
    "                self.vocab_size = metadata.get('vocab_size', 10)  # Default to 10 (0-9)\n",
    "                return metadata\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not load metadata: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "        \n",
    "    def validate_samples(self, num_samples=5):\n",
    "        \"\"\"Validate a subset of samples for data quality\"\"\"\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"❌ No samples to validate\")\n",
    "            return\n",
    "            \n",
    "        print(f\"\\n🔍 Validating {min(num_samples, len(self.samples))} random samples\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Check a few random samples\n",
    "        indices = np.random.choice(len(self.samples), min(num_samples, len(self.samples)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = self.samples[idx]\n",
    "            input_ids = sample['input_ids'].numpy()\n",
    "            target = sample['target'].numpy()\n",
    "            \n",
    "            # Check if non-zero inputs match targets\n",
    "            mask = input_ids != 0\n",
    "            matches = (input_ids[mask] == target[mask])\n",
    "            match_rate = matches.mean() if matches.size > 0 else 1.0\n",
    "            \n",
    "            # Check solution validity\n",
    "            is_valid = self._is_valid_sudoku(target.reshape(9, 9))\n",
    "            \n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\"  - Non-zero inputs match solution: {match_rate*100:.1f}%\")\n",
    "            print(f\"  - Solution is valid Sudoku: {is_valid}\")\n",
    "            if match_rate < 1.0:\n",
    "                print(f\"  - WARNING: Input clues don't match solution!\")\n",
    "                \n",
    "                # Print first few mismatches\n",
    "                mismatch_indices = np.where((input_ids != 0) & (input_ids != target))[0]\n",
    "                if len(mismatch_indices) > 0:\n",
    "                    for i in range(min(3, len(mismatch_indices))):\n",
    "                        idx = mismatch_indices[i]\n",
    "                        print(f\"    Position {idx}: Input={input_ids[idx]}, Solution={target[idx]}\")\n",
    "        \n",
    "        print(\"=\" * 40)\n",
    "\n",
    "# Basic Transformer model for Sudoku\n",
    "class SudokuTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for Sudoku solving\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=10, hidden_size=128, num_layers=4, num_heads=4, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Fixed positional encoding (simpler than the enhanced version)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(0, 81).expand((1, -1))\n",
    "        )\n",
    "        self.position_embedding = nn.Embedding(81, hidden_size)\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output head\n",
    "        self.ln_f = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Get position IDs\n",
    "        position_ids = self.position_ids[:, :input_ids.size(1)]\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        position_embeds = self.position_embedding(position_ids)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_embeds + position_embeds\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Utility functions\n",
    "def is_valid_sudoku(grid_flat):\n",
    "    \"\"\"Check if a flattened 9x9 grid is a valid Sudoku\"\"\"\n",
    "    if isinstance(grid_flat, torch.Tensor):\n",
    "        grid_flat = grid_flat.cpu().numpy()\n",
    "        \n",
    "    grid = grid_flat.reshape(9, 9)\n",
    "    \n",
    "    # Check rows\n",
    "    for i in range(9):\n",
    "        row = grid[i, :]\n",
    "        row_no_zeros = row[row != 0]\n",
    "        if len(row_no_zeros) != len(set(row_no_zeros)):\n",
    "            return False\n",
    "            \n",
    "    # Check columns\n",
    "    for i in range(9):\n",
    "        col = grid[:, i]\n",
    "        col_no_zeros = col[col != 0]\n",
    "        if len(col_no_zeros) != len(set(col_no_zeros)):\n",
    "            return False\n",
    "            \n",
    "    # Check 3x3 boxes\n",
    "    for box_row in range(3):\n",
    "        for box_col in range(3):\n",
    "            box = grid[box_row*3:(box_row+1)*3, box_col*3:(box_col+1)*3].flatten()\n",
    "            box_no_zeros = box[box != 0]\n",
    "            if len(box_no_zeros) != len(set(box_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "    return True\n",
    "\n",
    "def print_sudoku(grid, title=\"Sudoku Puzzle\"):\n",
    "    \"\"\"Pretty print a Sudoku grid\"\"\"\n",
    "    if isinstance(grid, torch.Tensor):\n",
    "        grid = grid.cpu().numpy()\n",
    "    \n",
    "    if len(grid.shape) == 1:  # Flatten to 9x9\n",
    "        grid = grid.reshape(9, 9)\n",
    "    \n",
    "    print(f\"\\n{title}:\")\n",
    "    for i in range(9):\n",
    "        if i % 3 == 0 and i > 0:\n",
    "            print(\"------+-------+------\")\n",
    "        row = \"\"\n",
    "        for j in range(9):\n",
    "            if j % 3 == 0 and j > 0:\n",
    "                row += \"| \"\n",
    "            val = grid[i, j].item() if hasattr(grid[i, j], 'item') else grid[i, j]\n",
    "            # Make sure we display valid Sudoku values (0-9)\n",
    "            if val > 9:\n",
    "                val = 9  # Cap at 9 for display\n",
    "            row += f\"{val if val != 0 else '.'} \"\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d40495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. ENVIRONMENT SETUP\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check for available devices\n",
    "device_name = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"🔍 Using device: {device_name.upper()}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Project paths\n",
    "ROOT_DIR = Path(\"/Users/robertburkhall/Development/HRM\")\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"sudoku-extreme-1k-aug-1000\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "MODEL_DIR = ROOT_DIR / \"models\"\n",
    "\n",
    "print(f\"📁 ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\"📁 DATA_DIR: {DATA_DIR}\")\n",
    "\n",
    "# Quick dataset file inspection\n",
    "def inspect_dataset_files(data_dir):\n",
    "    \"\"\"Directly inspect the dataset files without using the loader class\"\"\"\n",
    "    print(\"\\n🔍 DIRECT DATASET INSPECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = Path(data_dir) / split\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            print(f\"❌ {split} directory not found: {split_dir}\")\n",
    "            continue\n",
    "        \n",
    "        inputs_file = split_dir / \"all__inputs.npy\"\n",
    "        labels_file = split_dir / \"all__labels.npy\"\n",
    "        \n",
    "        if not inputs_file.exists() or not labels_file.exists():\n",
    "            print(f\"❌ Required files missing in {split}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load arrays\n",
    "            inputs = np.load(inputs_file)\n",
    "            labels = np.load(labels_file)\n",
    "            \n",
    "            print(f\"\\n✅ {split.upper()} Split:\")\n",
    "            print(f\"  - Inputs: {inputs.shape}, dtype={inputs.dtype}\")\n",
    "            print(f\"  - Labels: {labels.shape}, dtype={labels.dtype}\")\n",
    "            \n",
    "            # Check a random sample\n",
    "            if len(inputs) > 0:\n",
    "                idx = np.random.randint(0, len(inputs))\n",
    "                input_sample = inputs[idx]\n",
    "                label_sample = labels[idx]\n",
    "                \n",
    "                # Check clue consistency\n",
    "                non_zero_mask = input_sample > 0\n",
    "                clues_match = np.all(input_sample[non_zero_mask] == label_sample[non_zero_mask])\n",
    "                \n",
    "                print(f\"  - Random sample {idx}:\")\n",
    "                print(f\"    - Non-zero inputs: {np.sum(non_zero_mask)}\")\n",
    "                print(f\"    - Clues match solution: {'✅' if clues_match else '❌'}\")\n",
    "                \n",
    "                if not clues_match:\n",
    "                    mismatches = np.sum(input_sample[non_zero_mask] != label_sample[non_zero_mask])\n",
    "                    print(f\"    - Mismatches: {mismatches} positions\")\n",
    "                    \n",
    "                    # Show first few mismatches\n",
    "                    mismatch_indices = np.where((input_sample > 0) & (input_sample != label_sample))[0]\n",
    "                    for i, pos in enumerate(mismatch_indices[:3]):\n",
    "                        row, col = pos // 9, pos % 9\n",
    "                        print(f\"      Position ({row+1},{col+1}): Input={input_sample[pos]}, Solution={label_sample[pos]}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error inspecting {split} files: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_dataset_files(DATA_DIR)\n",
    "\n",
    "# Dataset Inspection\n",
    "# This cell examines the dataset structure and verifies data integrity\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check for available devices\n",
    "device_name = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"🔍 Using device: {device_name.upper()}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Project paths\n",
    "ROOT_DIR = Path(\"/Users/robertburkhall/Development/HRM\")\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"sudoku-extreme-1k-aug-1000\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "MODEL_DIR = ROOT_DIR / \"models\"\n",
    "\n",
    "print(f\"📁 ROOT_DIR: {ROOT_DIR}\")\n",
    "print(f\"📁 DATA_DIR: {DATA_DIR}\")\n",
    "\n",
    "# Quick dataset file inspection\n",
    "def inspect_dataset_files(data_dir):\n",
    "    \"\"\"Directly inspect the dataset files without using the loader class\"\"\"\n",
    "    print(\"\\n🔍 DIRECT DATASET INSPECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = Path(data_dir) / split\n",
    "        \n",
    "        if not split_dir.exists():\n",
    "            print(f\"❌ {split} directory not found: {split_dir}\")\n",
    "            continue\n",
    "        \n",
    "        inputs_file = split_dir / \"all__inputs.npy\"\n",
    "        labels_file = split_dir / \"all__labels.npy\"\n",
    "        \n",
    "        if not inputs_file.exists() or not labels_file.exists():\n",
    "            print(f\"❌ Required files missing in {split}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load arrays\n",
    "            inputs = np.load(inputs_file)\n",
    "            labels = np.load(labels_file)\n",
    "            \n",
    "            print(f\"\\n✅ {split.upper()} Split:\")\n",
    "            print(f\"  - Inputs: {inputs.shape}, dtype={inputs.dtype}\")\n",
    "            print(f\"  - Labels: {labels.shape}, dtype={labels.dtype}\")\n",
    "            \n",
    "            # Check a random sample\n",
    "            if len(inputs) > 0:\n",
    "                idx = np.random.randint(0, len(inputs))\n",
    "                input_sample = inputs[idx]\n",
    "                label_sample = labels[idx]\n",
    "                \n",
    "                # Check clue consistency\n",
    "                non_zero_mask = input_sample > 0\n",
    "                clues_match = np.all(input_sample[non_zero_mask] == label_sample[non_zero_mask])\n",
    "                \n",
    "                print(f\"  - Random sample {idx}:\")\n",
    "                print(f\"    - Non-zero inputs: {np.sum(non_zero_mask)}\")\n",
    "                print(f\"    - Clues match solution: {'✅' if clues_match else '❌'}\")\n",
    "                \n",
    "                if not clues_match:\n",
    "                    mismatches = np.sum(input_sample[non_zero_mask] != label_sample[non_zero_mask])\n",
    "                    print(f\"    - Mismatches: {mismatches} positions\")\n",
    "                    \n",
    "                    # Show first few mismatches\n",
    "                    mismatch_indices = np.where((input_sample > 0) & (input_sample != label_sample))[0]\n",
    "                    for i, pos in enumerate(mismatch_indices[:3]):\n",
    "                        row, col = pos // 9, pos % 9\n",
    "                        print(f\"      Position ({row+1},{col+1}): Input={input_sample[pos]}, Solution={label_sample[pos]}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error inspecting {split} files: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_dataset_files(DATA_DIR)\n",
    "\n",
    "#@title 1.1 Dataset Quick Check\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_sudoku_data():\n",
    "    \"\"\"Directly examine the Sudoku dataset to verify inputs and solutions match\"\"\"\n",
    "    data_path = Path(\"/Users/robertburkhall/Development/HRM/data/sudoku-extreme-1k-aug-1000\")\n",
    "    if not data_path.exists():\n",
    "        print(f\"❌ Dataset not found at: {data_path}\")\n",
    "        return False\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_path = data_path / split\n",
    "        if not split_path.exists():\n",
    "            print(f\"❌ {split} directory not found\")\n",
    "            continue\n",
    "        \n",
    "        inputs_file = split_path / \"all__inputs.npy\"\n",
    "        labels_file = split_path / \"all__labels.npy\"\n",
    "        \n",
    "        if not inputs_file.exists() or not labels_file.exists():\n",
    "            print(f\"❌ Missing input or label files in {split}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load a small sample of the data\n",
    "            inputs = np.load(inputs_file)\n",
    "            labels = np.load(labels_file)\n",
    "            \n",
    "            print(f\"\\n📊 {split} dataset stats:\")\n",
    "            print(f\"  • Samples: {inputs.shape[0]:,}\")\n",
    "            print(f\"  • Input shape: {inputs.shape}\")\n",
    "            print(f\"  • Label shape: {labels.shape}\")\n",
    "            print(f\"  • Input values range: {inputs.min()} to {inputs.max()}\")\n",
    "            print(f\"  • Label values range: {labels.min()} to {labels.max()}\")\n",
    "            \n",
    "            # Check if non-zero inputs match labels\n",
    "            sample_size = min(20, inputs.shape[0])\n",
    "            mismatches = 0\n",
    "            \n",
    "            # Try creating actual dataset samples as our HRMSudokuDataset would\n",
    "            print(f\"\\nCreating {sample_size} dataset samples as HRMSudokuDataset would:\")\n",
    "            for i in range(sample_size):\n",
    "                input_grid = inputs[i]\n",
    "                label_grid = labels[i]\n",
    "                \n",
    "                # Check if non-zero values in input match labels\n",
    "                mask = input_grid != 0\n",
    "                input_matches_solution = np.all(input_grid[mask] == label_grid[mask])\n",
    "                \n",
    "                if not input_matches_solution:\n",
    "                    mismatches += 1\n",
    "                    \n",
    "                    if mismatches <= 2:  # Only show first two mismatches\n",
    "                        print(f\"\\n❌ Mismatch in {split} sample {i}:\")\n",
    "                        mismatch_indices = np.where((input_grid != 0) & (input_grid != label_grid))[0]\n",
    "                        \n",
    "                        for idx in mismatch_indices[:5]:  # Show up to 5 mismatched positions\n",
    "                            print(f\"  Position {idx}: Input={input_grid[idx]}, Label={label_grid[idx]}\")\n",
    "                \n",
    "                # Create sample like HRMSudokuDataset would\n",
    "                sample = {\n",
    "                    'input_ids': torch.tensor(input_grid, dtype=torch.long),\n",
    "                    'target': torch.tensor(label_grid, dtype=torch.long)\n",
    "                }\n",
    "                \n",
    "                # Check if non-zero input values match target in the sample\n",
    "                sample_input = sample['input_ids'].numpy()\n",
    "                sample_target = sample['target'].numpy()\n",
    "                mask = sample_input != 0\n",
    "                sample_matches = np.all(sample_input[mask] == sample_target[mask])\n",
    "                \n",
    "                if not sample_matches:\n",
    "                    print(f\"❌ Dataset sample {i} has mismatches after conversion to torch tensors\")\n",
    "                \n",
    "                # Print a sample as grid (first one only)\n",
    "                if i == 0:\n",
    "                    print(f\"\\nExample from {split} dataset (sample {i}):\")\n",
    "                    print(\"Input puzzle:\")\n",
    "                    print_sudoku_grid(input_grid)\n",
    "                    print(\"\\nSolution:\")\n",
    "                    print_sudoku_grid(label_grid)\n",
    "                    \n",
    "                    # Print a few positions for verification\n",
    "                    print(\"\\nVerifying a few positions:\")\n",
    "                    for pos in [0, 10, 20, 30, 40]:\n",
    "                        has_clue = input_grid[pos] != 0\n",
    "                        matches = input_grid[pos] == label_grid[pos] if has_clue else True\n",
    "                        print(f\"Position {pos}: Input={input_grid[pos]}, Solution={label_grid[pos]}, Has clue: {has_clue}, Matches: {matches}\")\n",
    "            \n",
    "            if mismatches > 0:\n",
    "                print(f\"\\n❌ Found {mismatches}/{sample_size} samples with mismatches in {split}\")\n",
    "            else:\n",
    "                print(f\"\\n✅ All {sample_size} checked samples in {split} have consistent inputs and labels\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {split} data: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def is_valid_sudoku_solution(grid):\n",
    "    \"\"\"Check if a 9x9 grid is a valid Sudoku solution\"\"\"\n",
    "    # Check rows\n",
    "    for i in range(9):\n",
    "        if not is_valid_group(grid[i, :]):\n",
    "            return False\n",
    "    \n",
    "    # Check columns\n",
    "    for i in range(9):\n",
    "        if not is_valid_group(grid[:, i]):\n",
    "            return False\n",
    "    \n",
    "    # Check 3x3 boxes\n",
    "    for r in range(0, 9, 3):\n",
    "        for c in range(0, 9, 3):\n",
    "            if not is_valid_group(grid[r:r+3, c:c+3].flatten()):\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def is_valid_group(group):\n",
    "    \"\"\"Check if a group of 9 numbers is valid (no duplicates except 0)\"\"\"\n",
    "    # Remove zeros\n",
    "    non_zeros = group[group != 0]\n",
    "    # Check if all non-zero elements are unique\n",
    "    return len(non_zeros) == len(set(non_zeros))\n",
    "\n",
    "def print_sudoku_grid(grid):\n",
    "    \"\"\"Print a Sudoku grid in a readable format\"\"\"\n",
    "    grid = grid.reshape(9, 9)\n",
    "    for i in range(9):\n",
    "        if i % 3 == 0 and i > 0:\n",
    "            print(\"------+-------+------\")\n",
    "        row = \"\"\n",
    "        for j in range(9):\n",
    "            if j % 3 == 0 and j > 0:\n",
    "                row += \"| \"\n",
    "            val = grid[i, j]\n",
    "            row += f\"{val if val != 0 else '.'} \"\n",
    "        print(row)\n",
    "\n",
    "# Run the dataset check\n",
    "check_sudoku_data()\n",
    "def _save_config(self, button):\n",
    "        config = self._get_training_config()\n",
    "        config_name = self.config_name_text.value.strip()\n",
    "        if not config_name:\n",
    "            self.status_label.value = \"Please enter a config name before saving.\"\n",
    "            return\n",
    "        config_path = self.model_dir / f\"{config_name}.json\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        self.status_label.value = (\n",
    "            f\"Config '{config_name}' saved at: {config_path}\"\n",
    "        )\n",
    "        # Refresh config list and update dropdown after saving\n",
    "        self._refresh_config_list()\n",
    "        self.config_dropdown.options = self.config_list\n",
    "    \n",
    "def _setup_training_section(self):\n",
    "        # ...existing code...\n",
    "        self.save_config_button.on_click(self._save_config)\n",
    "        # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea38658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Dataset Implementation\n",
    "# This cell provides a more robust dataset implementation with validation\n",
    "\n",
    "class HRMSudokuDataset(Dataset):\n",
    "    \"\"\"Smart dataset loader for HRM Sudoku data format\"\"\"\n",
    "\n",
    "    def __init__(self, data_path, split='train', max_samples=100):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.split = split\n",
    "        self.samples = []\n",
    "        self.vocab_size = 10  # Using 0-9 for Sudoku (changed from 11)\n",
    "        self.debug_info = []  # Store debugging information\n",
    "\n",
    "        print(f\"\\n🔍 Loading HRM dataset from: {self.data_path / split}\")\n",
    "\n",
    "        split_dir = self.data_path / split\n",
    "        if not split_dir.exists():\n",
    "            print(f\"❌ Directory {split_dir} not found, creating synthetic data\")\n",
    "            self.samples = self._create_synthetic_samples(max_samples)\n",
    "            return\n",
    "\n",
    "        # Load metadata\n",
    "        metadata = self._load_metadata(split_dir)\n",
    "\n",
    "        # Try to directly load the numpy files we expect\n",
    "        inputs_file = split_dir / \"all__inputs.npy\"\n",
    "        labels_file = split_dir / \"all__labels.npy\"\n",
    "        \n",
    "        if inputs_file.exists() and labels_file.exists():\n",
    "            print(f\"✅ Found standard HRM format files:\")\n",
    "            print(f\"   - {inputs_file.name}\")\n",
    "            print(f\"   - {labels_file.name}\")\n",
    "            try:\n",
    "                inputs = np.load(inputs_file)\n",
    "                labels = np.load(labels_file)\n",
    "                \n",
    "                print(f\"📊 Loaded arrays - inputs: {inputs.shape}, labels: {labels.shape}\")\n",
    "                \n",
    "                if len(inputs) == len(labels):\n",
    "                    # Verify and add samples with validation\n",
    "                    valid_count = 0\n",
    "                    for i in range(min(len(inputs), max_samples)):\n",
    "                        if self._add_validated_sample(inputs[i], labels[i]):\n",
    "                            valid_count += 1\n",
    "                    \n",
    "                    print(f\"✅ Added {valid_count} validated samples from standard files\")\n",
    "                    if valid_count > 0:\n",
    "                        # Show sample validation\n",
    "                        self._verify_sample_consistency(3)\n",
    "                        return\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error loading standard files: {e}\")\n",
    "\n",
    "        # Find data files (non-JSON files)\n",
    "        data_files = [f for f in split_dir.iterdir() if f.suffix != '.json' and f.is_file()]\n",
    "        print(f\"📁 Found {len(data_files)} data files\")\n",
    "\n",
    "        # Try to load real data\n",
    "        loaded_samples = 0\n",
    "        for data_file in data_files[:min(len(data_files), 5)]:  # Limit to first 5 files\n",
    "            print(f\"🔍 Processing: {data_file.name}\")\n",
    "\n",
    "            success = (\n",
    "                self._try_numpy_loading(data_file, max_samples - loaded_samples) or\n",
    "                self._try_pickle_loading(data_file, max_samples - loaded_samples) or\n",
    "                self._try_binary_loading(data_file, metadata, max_samples - loaded_samples) or\n",
    "                self._try_text_loading(data_file, max_samples - loaded_samples)\n",
    "            )\n",
    "\n",
    "            if success:\n",
    "                loaded_samples = len(self.samples)\n",
    "                print(f\"  ✅ Loaded {loaded_samples} samples so far\")\n",
    "                if loaded_samples >= max_samples:\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"  ❌ Could not process {data_file.name}\")\n",
    "\n",
    "        # Fallback to synthetic data if nothing loaded\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"⚠️ No real data loaded, creating synthetic puzzles...\")\n",
    "            self.samples = self._create_synthetic_samples(max_samples)\n",
    "        else:\n",
    "            # Verify sample consistency\n",
    "            self._verify_sample_consistency(3)\n",
    "\n",
    "        print(f\"✅ Final dataset: {len(self.samples)} {split} samples\")\n",
    "\n",
    "    def _verify_sample_consistency(self, num_samples=3):\n",
    "        \"\"\"Verify and print sample consistency for debugging\"\"\"\n",
    "        if not self.samples:\n",
    "            return\n",
    "            \n",
    "        print(\"\\n🔍 DATASET VALIDATION:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Check a few random samples\n",
    "        indices = np.random.choice(len(self.samples), min(num_samples, len(self.samples)), replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = self.samples[idx]\n",
    "            input_ids = sample['input_ids'].numpy()\n",
    "            target = sample['target'].numpy()\n",
    "            \n",
    "            # Check if non-zero inputs match targets\n",
    "            mask = input_ids != 0\n",
    "            matches = (input_ids[mask] == target[mask])\n",
    "            match_rate = matches.mean() if matches.size > 0 else 1.0\n",
    "            \n",
    "            # Check solution validity\n",
    "            is_valid = self._is_valid_sudoku(target.reshape(9, 9))\n",
    "            \n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\"  - Non-zero inputs match solution: {match_rate*100:.1f}%\")\n",
    "            print(f\"  - Solution is valid Sudoku: {is_valid}\")\n",
    "            if match_rate < 1.0:\n",
    "                print(f\"  - WARNING: Input clues don't match solution!\")\n",
    "                \n",
    "                # Print first few mismatches\n",
    "                mismatch_indices = np.where((input_ids != 0) & (input_ids != target))[0]\n",
    "                if len(mismatch_indices) > 0:\n",
    "                    for i in range(min(3, len(mismatch_indices))):\n",
    "                        idx = mismatch_indices[i]\n",
    "                        print(f\"    Position {idx}: Input={input_ids[idx]}, Solution={target[idx]}\")\n",
    "        \n",
    "        print(\"=\" * 40)\n",
    "\n",
    "    def _is_valid_sudoku(self, grid):\n",
    "        \"\"\"Check if 9x9 grid is valid Sudoku solution\"\"\"\n",
    "        # Check rows\n",
    "        for i in range(9):\n",
    "            row = grid[i, :]\n",
    "            row_no_zeros = row[row != 0]\n",
    "            if len(row_no_zeros) != len(set(row_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "        # Check columns\n",
    "        for i in range(9):\n",
    "            col = grid[:, i]\n",
    "            col_no_zeros = col[col != 0]\n",
    "            if len(col_no_zeros) != len(set(col_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "        # Check 3x3 boxes\n",
    "        for box_row in range(3):\n",
    "            for box_col in range(3):\n",
    "                box = grid[box_row*3:(box_row+1)*3, box_col*3:(box_col+1)*3].flatten()\n",
    "                box_no_zeros = box[box != 0]\n",
    "                if len(box_no_zeros) != len(set(box_no_zeros)):\n",
    "                    return False\n",
    "                    \n",
    "        return True\n",
    "\n",
    "    def _load_metadata(self, split_dir):\n",
    "        \"\"\"Load metadata from dataset.json\"\"\"\n",
    "        metadata_file = split_dir / \"dataset.json\"\n",
    "        if metadata_file.exists():\n",
    "            try:\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                print(f\"📊 Metadata: vocab_size={metadata.get('vocab_size', 10)}\")\n",
    "                self.vocab_size = metadata.get('vocab_size', 10)  # Default to 10 (0-9)\n",
    "                return metadata\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not load metadata: {e}\")\n",
    "        return {}\n",
    "\n",
    "    def _try_numpy_loading(self, data_file, max_samples):\n",
    "        \"\"\"Try loading as numpy array\"\"\"\n",
    "        if data_file.suffix not in ['.npy', '.npz']:\n",
    "            return False\n",
    "        try:\n",
    "            data = np.load(data_file, allow_pickle=True)\n",
    "            return self._process_array_data(data, max_samples)\n",
    "        except Exception as e:\n",
    "            print(f\"  numpy load error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _try_pickle_loading(self, data_file, max_samples):\n",
    "        \"\"\"Try loading as pickle file\"\"\"\n",
    "        try:\n",
    "            import pickle\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return self._process_structured_data(data, max_samples)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _try_binary_loading(self, data_file, metadata, max_samples):\n",
    "        \"\"\"Try loading as binary data\"\"\"\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            seq_len = metadata.get('seq_len', 81)\n",
    "\n",
    "            # Try different integer formats\n",
    "            for dtype in [np.uint8, np.int32, np.int16]:\n",
    "                try:\n",
    "                    int_data = np.frombuffer(data, dtype=dtype)\n",
    "                    if len(int_data) >= seq_len * 2:  # At least one input+target pair\n",
    "                        pairs_per_sample = seq_len * 2\n",
    "                        num_samples = min(len(int_data) // pairs_per_sample, max_samples)\n",
    "\n",
    "                        for i in range(num_samples):\n",
    "                            start = i * pairs_per_sample\n",
    "                            input_data = int_data[start:start + seq_len]\n",
    "                            target_data = int_data[start + seq_len:start + pairs_per_sample]\n",
    "\n",
    "                            # Add with validation\n",
    "                            self._add_validated_sample(input_data, target_data)\n",
    "\n",
    "                        return len(self.samples) > 0\n",
    "                except:\n",
    "                    continue\n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _try_text_loading(self, data_file, max_samples):\n",
    "        \"\"\"Try loading as text file\"\"\"\n",
    "        try:\n",
    "            with open(data_file, 'r') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Try JSON first\n",
    "            try:\n",
    "                data = json.loads(content)\n",
    "                return self._process_structured_data(data, max_samples)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Try parsing numbers\n",
    "            lines = content.strip().split('\\n')\n",
    "            for line in lines[:max_samples]:\n",
    "                numbers = []\n",
    "                for part in line.replace(',', ' ').split():\n",
    "                    try:\n",
    "                        numbers.append(int(part))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                if len(numbers) == 162:  # 81 input + 81 target\n",
    "                    self._add_validated_sample(numbers[:81], numbers[81:])\n",
    "                elif len(numbers) == 81:\n",
    "                    # Just input, create dummy target\n",
    "                    self._add_validated_sample(numbers, numbers)\n",
    "\n",
    "            return len(self.samples) > 0\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _process_array_data(self, data, max_samples):\n",
    "        \"\"\"Process numpy array data\"\"\"\n",
    "        try:\n",
    "            if isinstance(data, np.ndarray):\n",
    "                if data.ndim == 3 and data.shape[-1] == 81:\n",
    "                    # [num_samples, 2, 81] format\n",
    "                    for i in range(min(data.shape[0], max_samples)):\n",
    "                        if data.shape[1] >= 2:\n",
    "                            self._add_validated_sample(data[i, 0], data[i, 1])\n",
    "                elif data.ndim == 2 and data.shape[-1] == 162:\n",
    "                    # [num_samples, 162] format\n",
    "                    for i in range(min(data.shape[0], max_samples)):\n",
    "                        self._add_validated_sample(data[i, :81], data[i, 81:])\n",
    "            return len(self.samples) > 0\n",
    "        except Exception as e:\n",
    "            print(f\"  array processing error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _process_structured_data(self, data, max_samples):\n",
    "        \"\"\"Process structured data (lists, dicts)\"\"\"\n",
    "        try:\n",
    "            if isinstance(data, (list, tuple)):\n",
    "                for item in data[:max_samples]:\n",
    "                    if isinstance(item, dict):\n",
    "                        input_data = item.get('input') or item.get('puzzle') or item.get('problem')\n",
    "                        target_data = item.get('target') or item.get('solution') or item.get('answer')\n",
    "                        if input_data is not None and target_data is not None:\n",
    "                            self._add_validated_sample(input_data, target_data)\n",
    "            elif isinstance(data, dict):\n",
    "                if 'input' in data and 'target' in data:\n",
    "                    self._add_validated_sample(data['input'], data['target'])\n",
    "            return len(self.samples) > 0\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _add_validated_sample(self, input_data, target_data):\n",
    "        \"\"\"Add a sample with validation to ensure input/solution consistency\"\"\"\n",
    "        try:\n",
    "            input_array = np.array(input_data, dtype=np.int64)\n",
    "            target_array = np.array(target_data, dtype=np.int64)\n",
    "\n",
    "            # Cap values at 9 for Sudoku\n",
    "            input_array = np.clip(input_array, 0, 9)\n",
    "            target_array = np.clip(target_array, 0, 9)\n",
    "\n",
    "            if not (len(input_array) == 81 and len(target_array) == 81):\n",
    "                return False\n",
    "\n",
    "            if not (np.all(input_array >= 0) and np.all(input_array < self.vocab_size) and\n",
    "                   np.all(target_array >= 0) and np.all(target_array < self.vocab_size)):\n",
    "                return False\n",
    "\n",
    "            # CRITICAL: Ensure all non-zero input values match the target values\n",
    "            # This is essential for valid Sudoku puzzles\n",
    "            non_zero_mask = input_array > 0\n",
    "            if not np.all(input_array[non_zero_mask] == target_array[non_zero_mask]):\n",
    "                return False\n",
    "                \n",
    "            # Validate solution is a proper Sudoku grid\n",
    "            if not self._is_valid_sudoku(target_array.reshape(9, 9)):\n",
    "                return False\n",
    "\n",
    "            self.samples.append({\n",
    "                'input_ids': torch.tensor(input_array, dtype=torch.long),\n",
    "                'target': torch.tensor(target_array, dtype=torch.long)\n",
    "            })\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    def _create_synthetic_samples(self, num_samples):\n",
    "        \"\"\"Create synthetic Sudoku samples\"\"\"\n",
    "        samples = []\n",
    "\n",
    "        # High-quality Sudoku puzzle for demo\n",
    "        base_puzzle = {\n",
    "            'input': [5,3,0,0,7,0,0,0,0,6,0,0,1,9,5,0,0,0,0,9,8,0,0,0,0,6,0,8,0,0,0,6,0,0,0,3,4,0,0,8,0,3,0,0,1,7,0,0,0,2,0,0,0,6,0,6,0,0,0,0,2,8,0,0,0,0,4,1,9,0,0,5,0,0,0,0,8,0,0,7,9],\n",
    "            'target': [5,3,4,6,7,8,9,1,2,6,7,2,1,9,5,3,4,8,1,9,8,3,4,2,5,6,7,8,5,9,7,6,1,4,2,3,4,2,6,8,5,3,7,9,1,7,1,3,9,2,4,8,5,6,9,6,1,5,3,7,2,8,4,2,8,7,4,1,9,6,3,5,3,4,5,2,8,6,1,7,9]\n",
    "        }\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            input_data = base_puzzle['input'].copy()\n",
    "            target_data = base_puzzle['target'].copy()\n",
    "\n",
    "            # Add variation by removing more clues\n",
    "            if i > 0:\n",
    "                non_zero_indices = [idx for idx, val in enumerate(input_data) if val != 0]\n",
    "                if non_zero_indices:\n",
    "                    remove_count = min(3 + i % 8, len(non_zero_indices) // 2)\n",
    "                    indices_to_zero = np.random.choice(non_zero_indices, size=remove_count, replace=False)\n",
    "                    for idx in indices_to_zero:\n",
    "                        input_data[idx] = 0\n",
    "\n",
    "            samples.append({\n",
    "                'input_ids': torch.tensor(input_data, dtype=torch.long),\n",
    "                'target': torch.tensor(target_data, dtype=torch.long)\n",
    "            })\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "#@title 🚀 Quick Start Verification (Run this to verify everything works)\n",
    "\n",
    "print(\"🔍 Running quick verification test...\")\n",
    "\n",
    "# Check essential components are defined\n",
    "essential_components = ['device', 'DATA_DIR', 'HRMSudokuDataset', 'SudokuTransformer']\n",
    "missing_components = []\n",
    "\n",
    "for component in essential_components:\n",
    "    if component not in globals():\n",
    "        missing_components.append(component)\n",
    "\n",
    "if missing_components:\n",
    "    print(f\"❌ Missing essential components: {', '.join(missing_components)}\")\n",
    "    print(\"   Please run cells #8 and #9 first to define these components.\")\n",
    "else:\n",
    "    print(\"✅ All essential components defined\")\n",
    "\n",
    "    # Create a small test dataset\n",
    "    try:\n",
    "        print(\"\\n📂 Testing dataset loading...\")\n",
    "        test_mini_dataset = HRMSudokuDataset(DATA_DIR, split='test', max_samples=3)\n",
    "        if len(test_mini_dataset) > 0:\n",
    "            print(f\"✅ Successfully loaded {len(test_mini_dataset)} test samples\")\n",
    "            \n",
    "            # Display info about first sample\n",
    "            sample = test_mini_dataset[0]\n",
    "            clue_count = (sample['input_ids'] > 0).sum().item()\n",
    "            print(f\"   Sample info: {clue_count} clues, shape: {sample['input_ids'].shape}\")\n",
    "        else:\n",
    "            print(\"⚠️ Dataset loaded but contains no samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading dataset: {str(e)}\")\n",
    "\n",
    "    # Create a tiny model\n",
    "    try:\n",
    "        print(\"\\n🧠 Testing model creation...\")\n",
    "        test_tiny_model = SudokuTransformer(\n",
    "            vocab_size=10, \n",
    "            hidden_size=32,  # Very small for quick testing\n",
    "            num_layers=2,\n",
    "            num_heads=2\n",
    "        ).to(device)\n",
    "        print(f\"✅ Model created with {sum(p.numel() for p in test_tiny_model.parameters()):,} parameters\")\n",
    "        \n",
    "        # Try a forward pass if we have data\n",
    "        if 'test_mini_dataset' in locals() and len(test_mini_dataset) > 0:\n",
    "            with torch.no_grad():\n",
    "                input_ids = test_mini_dataset[0]['input_ids'].to(device).unsqueeze(0)\n",
    "                output = test_tiny_model(input_ids)\n",
    "                print(f\"✅ Forward pass successful, output shape: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating/testing model: {str(e)}\")\n",
    "        \n",
    "print(\"\\n📋 Next steps:\")\n",
    "print(\"1. To run a mini training session → Run cell #12\")\n",
    "print(\"2. To test on a custom puzzle → Run cell #26\")\n",
    "print(\"3. To analyze model performance → Run cell #13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Verification Test\n",
    "# This cell performs a simple test to verify all components are working\n",
    "\n",
    "print(\"🔍 Running quick verification test...\")\n",
    "\n",
    "# Check essential components are defined\n",
    "essential_components = ['device', 'DATA_DIR', 'HRMSudokuDataset', 'SudokuTransformer']\n",
    "missing_components = []\n",
    "\n",
    "for component in essential_components:\n",
    "    if component not in globals():\n",
    "        missing_components.append(component)\n",
    "\n",
    "if missing_components:\n",
    "    print(f\"❌ Missing essential components: {', '.join(missing_components)}\")\n",
    "    print(\"   Please run the Environment Check and Model & Dataset Setup cells first to define these components.\")\n",
    "else:\n",
    "    print(\"✅ All essential components defined\")\n",
    "\n",
    "    # Create a small test dataset\n",
    "    try:\n",
    "        print(\"\\n📂 Testing dataset loading...\")\n",
    "        test_mini_dataset = HRMSudokuDataset(DATA_DIR, split='test', max_samples=3)\n",
    "        if len(test_mini_dataset) > 0:\n",
    "            print(f\"✅ Successfully loaded {len(test_mini_dataset)} test samples\")\n",
    "            \n",
    "            # Display info about first sample\n",
    "            sample = test_mini_dataset[0]\n",
    "            clue_count = (sample['input_ids'] > 0).sum().item()\n",
    "            print(f\"   Sample info: {clue_count} clues, shape: {sample['input_ids'].shape}\")\n",
    "        else:\n",
    "            print(\"⚠️ Dataset loaded but contains no samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading dataset: {str(e)}\")\n",
    "\n",
    "    # Create a tiny model\n",
    "    try:\n",
    "        print(\"\\n🧠 Testing model creation...\")\n",
    "        test_tiny_model = SudokuTransformer(\n",
    "            vocab_size=10, \n",
    "            hidden_size=32,  # Very small for quick testing\n",
    "            num_layers=2,\n",
    "            num_heads=2\n",
    "        ).to(device)\n",
    "        print(f\"✅ Model created with {sum(p.numel() for p in test_tiny_model.parameters()):,} parameters\")\n",
    "        \n",
    "        # Try a forward pass if we have data\n",
    "        if 'test_mini_dataset' in locals() and len(test_mini_dataset) > 0:\n",
    "            with torch.no_grad():\n",
    "                input_ids = test_mini_dataset[0]['input_ids'].to(device).unsqueeze(0)\n",
    "                output = test_tiny_model(input_ids)\n",
    "                print(f\"✅ Forward pass successful, output shape: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating/testing model: {str(e)}\")\n",
    "        \n",
    "print(\"\\n📋 Next steps:\")\n",
    "print(\"1. To run a mini training session → Run the Mini Training Loop cell\")\n",
    "print(\"2. To test on a custom puzzle → Run the Custom Puzzle Test cell\")\n",
    "print(\"3. To analyze model performance → Run the Model Diagnostics cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e17348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. MODEL DEFINITION\n",
    "\n",
    "class SudokuTransformer(nn.Module):\n",
    "    \"\"\"Transformer model for Sudoku solving - MacOS/MPS optimized\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=10, hidden_size=256, num_layers=4, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(81, hidden_size)  # 9x9 Sudoku\n",
    "\n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        # Position indices\n",
    "        pos_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        # Embeddings\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(pos_ids)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class EnhancedSudokuTransformer(nn.Module):\n",
    "    \"\"\"Enhanced Transformer model for Sudoku solving with grid-aware positional encoding\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=10, hidden_size=256, num_layers=4, num_heads=8, dropout=0.1, attention_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Separate embeddings for row, column, and box positions to better represent Sudoku structure\n",
    "        self.row_embedding = nn.Embedding(9, hidden_size // 3)\n",
    "        self.col_embedding = nn.Embedding(9, hidden_size // 3)\n",
    "        self.box_embedding = nn.Embedding(9, hidden_size // 3)\n",
    "        \n",
    "        # Projection layer to combine the position embeddings\n",
    "        self.pos_projection = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Transformer layers with norm_first for better training dynamics\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Apply layer norm before attention (more stable)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Calculate Sudoku grid positions\n",
    "        positions = torch.arange(81, device=device)\n",
    "        rows = positions // 9\n",
    "        cols = positions % 9\n",
    "        boxes = (rows // 3) * 3 + (cols // 3)  # Box index (0-8)\n",
    "        \n",
    "        # Expand for batch dimension\n",
    "        rows = rows.unsqueeze(0).expand(batch_size, -1)\n",
    "        cols = cols.unsqueeze(0).expand(batch_size, -1)\n",
    "        boxes = boxes.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Get embeddings\n",
    "        row_emb = self.row_embedding(rows)\n",
    "        col_emb = self.col_embedding(cols)\n",
    "        box_emb = self.box_embedding(boxes)\n",
    "        \n",
    "        # Concatenate position embeddings\n",
    "        pos_emb = torch.cat([row_emb, col_emb, box_emb], dim=-1)\n",
    "        pos_emb = self.pos_projection(pos_emb)\n",
    "        \n",
    "        # Combine with token embeddings\n",
    "        x = self.token_embedding(input_ids) + pos_emb\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Output\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "def print_sudoku(puzzle, title=\"Sudoku Puzzle\"):\n",
    "    \"\"\"Print a Sudoku puzzle with grid lines\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"-\" * 25)\n",
    "    for i in range(9):\n",
    "        row = puzzle[i*9:(i+1)*9]\n",
    "        row_str = \"\"\n",
    "        for j, val in enumerate(row):\n",
    "            if j % 3 == 0:\n",
    "                row_str += \"| \"\n",
    "            # Cap the value at 9 to ensure valid Sudoku display\n",
    "            val_num = val.item() if hasattr(val, 'item') else val\n",
    "            val_num = min(val_num, 9)  # Cap at 9 for display\n",
    "            row_str += f\"{int(val_num) if val_num > 0 else '.'} \"\n",
    "        row_str += \"|\"\n",
    "        print(row_str)\n",
    "        if i % 3 == 2:\n",
    "            print(\"-\" * 25)\n",
    "\n",
    "def is_valid_sudoku(grid_flat):\n",
    "    \"\"\"Check if a flattened 9x9 grid is a valid Sudoku (no duplicates in rows/cols/boxes)\"\"\"\n",
    "    grid = grid_flat.reshape(9, 9)\n",
    "    \n",
    "    # Check rows\n",
    "    for i in range(9):\n",
    "        row = grid[i, :]\n",
    "        row_no_zeros = row[row != 0]\n",
    "        if len(row_no_zeros) != len(set(row_no_zeros)):\n",
    "            return False\n",
    "            \n",
    "    # Check columns\n",
    "    for i in range(9):\n",
    "        col = grid[:, i]\n",
    "        col_no_zeros = col[col != 0]\n",
    "        if len(col_no_zeros) != len(set(col_no_zeros)):\n",
    "            return False\n",
    "            \n",
    "    # Check 3x3 boxes\n",
    "    for box_row in range(3):\n",
    "        for box_col in range(3):\n",
    "            box = grid[box_row*3:(box_row+1)*3, box_col*3:(box_col+1)*3].flatten()\n",
    "            box_no_zeros = box[box != 0]\n",
    "            if len(box_no_zeros) != len(set(box_no_zeros)):\n",
    "                return False\n",
    "                \n",
    "    return True\n",
    "\n",
    "def validate_puzzle_solution_pair(input_puzzle, solution):\n",
    "    \"\"\"Validate that a puzzle-solution pair is consistent and valid\"\"\"\n",
    "    # Check shapes\n",
    "    if input_puzzle.shape != solution.shape or len(input_puzzle) != 81:\n",
    "        return False, \"Invalid shape - should be 81 elements\"\n",
    "    \n",
    "    # Check clue consistency - all non-zero input values must match solution\n",
    "    non_zero_mask = input_puzzle > 0\n",
    "    if not np.all(input_puzzle[non_zero_mask] == solution[non_zero_mask]):\n",
    "        mismatches = np.sum(input_puzzle[non_zero_mask] != solution[non_zero_mask])\n",
    "        return False, f\"Input clues don't match solution in {mismatches} positions\"\n",
    "    \n",
    "    # Check solution validity\n",
    "    if not is_valid_sudoku(solution):\n",
    "        return False, \"Solution is not a valid Sudoku (has duplicates)\"\n",
    "    \n",
    "    return True, \"Valid puzzle-solution pair\"\n",
    "\n",
    "# Test with a specific sample\n",
    "print(\"Testing dataset...\")\n",
    "test_dataset = HRMSudokuDataset(DATA_DIR, split=\"test\", max_samples=50)\n",
    "train_dataset = HRMSudokuDataset(DATA_DIR, split=\"train\", max_samples=50)\n",
    "\n",
    "if len(test_dataset) > 0:\n",
    "    # Get a random sample for visualization\n",
    "    idx = np.random.randint(0, len(test_dataset))\n",
    "    sample = test_dataset[idx]\n",
    "    \n",
    "    input_puzzle = sample['input_ids'].numpy()\n",
    "    solution = sample['target'].numpy()\n",
    "    \n",
    "    # Validate the puzzle-solution pair\n",
    "    is_valid, message = validate_puzzle_solution_pair(input_puzzle, solution)\n",
    "    \n",
    "    # Display the validation result\n",
    "    print(f\"\\n{'✅' if is_valid else '❌'} Validation: {message}\")\n",
    "    \n",
    "    # Print both puzzles\n",
    "    print_sudoku(input_puzzle, \"Input Puzzle\")\n",
    "    print_sudoku(solution, \"Correct Solution\")\n",
    "    \n",
    "    # Show detailed validation info\n",
    "    if not is_valid:\n",
    "        print(\"\\nDetailed validation info:\")\n",
    "        # Show input validity\n",
    "        print(f\"Input is valid Sudoku: {is_valid_sudoku(input_puzzle)}\")\n",
    "        \n",
    "        # Show mismatched positions\n",
    "        non_zero_mask = input_puzzle > 0\n",
    "        if not np.all(input_puzzle[non_zero_mask] == solution[non_zero_mask]):\n",
    "            mismatches = np.where((input_puzzle > 0) & (input_puzzle != solution))[0]\n",
    "            print(f\"Mismatched positions (max 5): {len(mismatches)}\")\n",
    "            for i, pos in enumerate(mismatches[:5]):\n",
    "                row, col = pos // 9, pos % 9\n",
    "                print(f\"  Position ({row+1},{col+1}): Input={input_puzzle[pos]}, Solution={solution[pos]}\")\n",
    "else:\n",
    "    print(\"❌ No test samples available!\")\n",
    "\n",
    "#@title Minimal Training Test\n",
    "\n",
    "# Configure a very small training run to test the full pipeline\n",
    "mini_config = {\n",
    "    'epochs': 2,            # Very few epochs for testing\n",
    "    'batch_size': 8,        # Small batch size\n",
    "    'learning_rate': 7e-5,  # Standard learning rate\n",
    "    'max_train_samples': 20, # Very small dataset for quick testing\n",
    "    'max_val_samples': 10,   # Very small validation set\n",
    "    'log_every': 5,         # Log frequently \n",
    "    'validate_every': 10,   # Validate frequently\n",
    "}\n",
    "\n",
    "print(\"🔬 Running minimal training test...\")\n",
    "\n",
    "# Create small datasets for testing\n",
    "train_dataset = HRMSudokuDataset(DATA_DIR, split='train', max_samples=mini_config['max_train_samples'])\n",
    "val_dataset = HRMSudokuDataset(DATA_DIR, split='test', max_samples=mini_config['max_val_samples'])\n",
    "\n",
    "print(f\"📊 Training set: {len(train_dataset)} samples\")\n",
    "print(f\"📊 Validation set: {len(val_dataset)} samples\")\n",
    "\n",
    "if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "    print(\"❌ Not enough data for training test\")\n",
    "else:\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=mini_config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=mini_config['batch_size'])\n",
    "    \n",
    "    # Create model\n",
    "    model = SudokuTransformer(\n",
    "        vocab_size=10,\n",
    "        hidden_size=64,  # Small for fast testing\n",
    "        num_layers=2,    # Small for fast testing\n",
    "        num_heads=2      # Small for fast testing\n",
    "    ).to(device)\n",
    "    \n",
    "    # Create optimizer and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=mini_config['learning_rate'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding/zeros\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n🚀 Starting mini training...\")\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(mini_config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{mini_config['epochs']}\")\n",
    "        \n",
    "        # Training\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Get data\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Logging\n",
    "            global_step += 1\n",
    "            if global_step % mini_config['log_every'] == 0:\n",
    "                avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                print(f\"  Step {global_step}: Loss = {avg_loss:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            if global_step % mini_config['validate_every'] == 0:\n",
    "                # Run quick validation\n",
    "                model.eval()\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                val_exact_match = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_targets = val_batch['target'].to(device)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        val_logits = model(val_input_ids)\n",
    "                        val_preds = val_logits.argmax(dim=-1)\n",
    "                        \n",
    "                        # Compute metrics (only on non-zero targets)\n",
    "                        mask = val_targets != 0\n",
    "                        val_correct += (val_preds[mask] == val_targets[mask]).sum().item()\n",
    "                        val_total += mask.sum().item()\n",
    "                        \n",
    "                        # Check for exact matches\n",
    "                        for i in range(val_input_ids.size(0)):\n",
    "                            # Make sure to preserve the original clues\n",
    "                            sample_input = val_input_ids[i]\n",
    "                            sample_pred = val_preds[i].clone()\n",
    "                            sample_target = val_targets[i]\n",
    "                            \n",
    "                            # Force clues to match input\n",
    "                            non_zero_mask = sample_input > 0\n",
    "                            sample_pred[non_zero_mask] = sample_input[non_zero_mask]\n",
    "                            \n",
    "                            # Check if exactly matches target\n",
    "                            if torch.all(sample_pred == sample_target):\n",
    "                                val_exact_match += 1\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "                exact_match_rate = val_exact_match / len(val_dataset) if len(val_dataset) > 0 else 0\n",
    "                \n",
    "                print(f\"  Validation: Accuracy = {accuracy:.4f}, Exact Match = {exact_match_rate:.4f}\")\n",
    "                \n",
    "                # Switch back to training mode\n",
    "                model.train()\n",
    "    \n",
    "    print(\"\\n✅ Mini training complete!\")\n",
    "    \n",
    "    # Plot the training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Test inference on a sample\n",
    "    print(\"\\n🔍 Testing inference on a sample...\")\n",
    "    if len(val_dataset) > 0:\n",
    "        sample = val_dataset[0]\n",
    "        input_ids = sample['input_ids'].to(device)\n",
    "        target = sample['target']\n",
    "        \n",
    "        # Model prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids.unsqueeze(0))\n",
    "            pred = logits.argmax(dim=-1).squeeze().cpu()\n",
    "            \n",
    "            # Ensure we preserve the input clues in the output\n",
    "            input_clues = input_ids.cpu()\n",
    "            non_zero_mask = input_clues > 0\n",
    "            pred[non_zero_mask] = input_clues[non_zero_mask]\n",
    "        \n",
    "        # Print the results\n",
    "        print_sudoku(input_ids, title=\"Input Puzzle\")\n",
    "        print_sudoku(pred, title=\"Model Solution\")\n",
    "        print_sudoku(target, title=\"Ground Truth\")\n",
    "        \n",
    "        # Check if solution is valid\n",
    "        is_valid = is_valid_sudoku(pred)\n",
    "        matches_ground_truth = torch.all(pred == target).item()\n",
    "        \n",
    "        print(f\"Solution is valid Sudoku: {'✅' if is_valid else '❌'}\")\n",
    "        print(f\"Solution matches ground truth: {'✅' if matches_ground_truth else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88185a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. TRAINING FUNCTION\n",
    "\n",
    "def train_model(config):\n",
    "    \"\"\"Train the Sudoku model\"\"\"\n",
    "    print(f\"\\n🚀 Starting Training\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Use the global device\n",
    "    global device\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = HRMSudokuDataset(config['data_path'], 'train', config['max_train_samples'])\n",
    "    val_dataset = HRMSudokuDataset(config['data_path'], 'test', config['max_val_samples'])\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"❌ No training data available\")\n",
    "        return None\n",
    "\n",
    "    # Data loaders - reduce num_workers for macOS\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    model = SudokuTransformer(\n",
    "        vocab_size=train_dataset.vocab_size,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        num_heads=config['num_heads']\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"📊 Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"📊 Training on {len(train_dataset)} samples\")\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Training\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"epochs\"]}')\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "\n",
    "                logits = model(input_ids)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "\n",
    "                mask = targets != 0\n",
    "                val_correct += ((predictions == targets) & mask).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "\n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    return model, train_dataset, val_dataset\n",
    "\n",
    "#@title Model Diagnostics and Analysis\n",
    "\n",
    "# Configure diagnostics\n",
    "diagnostics_config = {\n",
    "    'num_samples': 5,  # Number of samples to analyze\n",
    "    'plot_error_heatmap': True,  # Whether to plot error heatmap\n",
    "    'analyze_difficulty': True,  # Whether to analyze cell difficulty\n",
    "}\n",
    "\n",
    "print(\"🔍 Running model diagnostics...\")\n",
    "\n",
    "def analyze_position_difficulty(model, dataset, device, num_samples=None):\n",
    "    \"\"\"Analyze which positions in the grid are most difficult for the model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Use all samples or a subset\n",
    "    if num_samples is None or num_samples > len(dataset):\n",
    "        num_samples = len(dataset)\n",
    "    \n",
    "    # Track errors by position\n",
    "    error_counts = np.zeros((9, 9))\n",
    "    total_counts = np.zeros((9, 9))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].to(device)\n",
    "            target = sample['target'].cpu()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids.unsqueeze(0))\n",
    "            pred = logits.argmax(dim=-1).squeeze().cpu()\n",
    "            \n",
    "            # Identify non-clue positions (where we need to predict)\n",
    "            non_clue_mask = (input_ids.cpu() == 0)\n",
    "            \n",
    "            # Check which positions are correct\n",
    "            errors = (pred != target) & non_clue_mask\n",
    "            \n",
    "            # Update counts\n",
    "            error_counts += errors.reshape(9, 9).numpy()\n",
    "            total_counts += non_clue_mask.reshape(9, 9).numpy()\n",
    "    \n",
    "    # Calculate error rates (avoid division by zero)\n",
    "    error_rates = np.zeros((9, 9))\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if total_counts[i, j] > 0:\n",
    "                error_rates[i, j] = error_counts[i, j] / total_counts[i, j]\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    heatmap = plt.imshow(error_rates, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "    plt.colorbar(heatmap, label='Error Rate')\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(10):\n",
    "        lw = 2 if i % 3 == 0 else 0.5\n",
    "        plt.axhline(i - 0.5, color='black', linewidth=lw)\n",
    "        plt.axvline(i - 0.5, color='black', linewidth=lw)\n",
    "    \n",
    "    # Add position labels\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if error_rates[i, j] > 0:\n",
    "                plt.text(j, i, f'{error_rates[i, j]:.2f}', \n",
    "                        ha='center', va='center', \n",
    "                        color='white' if error_rates[i, j] > 0.5 else 'black',\n",
    "                        fontsize=8)\n",
    "    \n",
    "    plt.title('Error Rate by Position')\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Row')\n",
    "    plt.xticks(range(9), range(1, 10))\n",
    "    plt.yticks(range(9), range(1, 10))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf(), error_rates\n",
    "\n",
    "def plot_error_heatmap(model, sample, device):\n",
    "    \"\"\"Create a heatmap showing where the model makes errors in a specific sample\"\"\"\n",
    "    input_grid = sample['input_ids'].to(device)\n",
    "    target_grid = sample['target'].cpu().numpy().reshape(9, 9)\n",
    "    \n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_grid.unsqueeze(0))\n",
    "        pred = logits.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Ensure clues are preserved\n",
    "        input_np = input_grid.cpu().numpy()\n",
    "        non_zero_mask = input_np > 0\n",
    "        pred[non_zero_mask] = input_np[non_zero_mask]\n",
    "        \n",
    "    pred_grid = pred.reshape(9, 9)\n",
    "    \n",
    "    # Create error mask (1 for error, 0 for correct)\n",
    "    errors = (pred_grid != target_grid).astype(int)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot input grid with errors highlighted\n",
    "    grid_display = np.zeros((9, 9, 3))\n",
    "    \n",
    "    # Color coding:\n",
    "    # - White (1,1,1): Empty cells in input\n",
    "    # - Blue (0.8,0.8,1): Given clues\n",
    "    # - Green (0.8,1,0.8): Correctly filled by model\n",
    "    # - Red (1,0.8,0.8): Incorrectly filled by model\n",
    "    \n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if input_np.reshape(9, 9)[i, j] > 0:\n",
    "                # Blue for given clues\n",
    "                grid_display[i, j] = [0.8, 0.8, 1.0]\n",
    "            elif errors[i, j] == 1:\n",
    "                # Red for errors\n",
    "                grid_display[i, j] = [1.0, 0.8, 0.8]\n",
    "            else:\n",
    "                # Green for correct predictions\n",
    "                grid_display[i, j] = [0.8, 1.0, 0.8]\n",
    "    \n",
    "    plt.imshow(grid_display)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(10):\n",
    "        lw = 2 if i % 3 == 0 else 0.5\n",
    "        plt.axhline(i - 0.5, color='black', linewidth=lw)\n",
    "        plt.axvline(i - 0.5, color='black', linewidth=lw)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            if input_np.reshape(9, 9)[i, j] > 0:\n",
    "                # Input clues\n",
    "                plt.text(j, i, str(int(input_np.reshape(9, 9)[i, j])), \n",
    "                        ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "            else:\n",
    "                # Model predictions (with error marking)\n",
    "                if errors[i, j] == 1:\n",
    "                    # Show both prediction and target for errors\n",
    "                    plt.text(j, i, f\"{int(pred_grid[i, j])}→{int(target_grid[i, j])}\", \n",
    "                            ha='center', va='center', color='darkred', fontsize=10)\n",
    "                else:\n",
    "                    # Just show prediction for correct cells\n",
    "                    plt.text(j, i, str(int(pred_grid[i, j])), \n",
    "                            ha='center', va='center', fontsize=11)\n",
    "    \n",
    "    plt.title('Model Prediction Analysis\\nBlue: Given clues | Green: Correct predictions | Red: Errors')\n",
    "    plt.xticks(range(9), range(1, 10))\n",
    "    plt.yticks(range(9), range(1, 10))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Run diagnostics if we have a model and dataset\n",
    "if 'model' not in locals() or 'val_dataset' not in locals():\n",
    "    print(\"❌ Model or validation dataset not found. Please run the training cell first.\")\n",
    "else:\n",
    "    # Run sample analysis\n",
    "    if len(val_dataset) > 0:\n",
    "        # Plot error heatmap for a sample\n",
    "        if diagnostics_config['plot_error_heatmap']:\n",
    "            print(\"\\n📊 Analyzing model errors on a sample...\")\n",
    "            sample_idx = 0\n",
    "            sample = val_dataset[sample_idx]\n",
    "            error_fig = plot_error_heatmap(model, sample, device)\n",
    "            plt.show()\n",
    "        \n",
    "        # Analyze position difficulty\n",
    "        if diagnostics_config['analyze_difficulty'] and len(val_dataset) >= 3:\n",
    "            print(\"\\n📊 Analyzing position difficulty across multiple samples...\")\n",
    "            difficulty_fig, error_rates = analyze_position_difficulty(\n",
    "                model, val_dataset, device, \n",
    "                num_samples=min(diagnostics_config['num_samples'], len(val_dataset))\n",
    "            )\n",
    "            plt.show()\n",
    "            \n",
    "            # Print the most difficult positions\n",
    "            print(\"\\nMost difficult positions (highest error rates):\")\n",
    "            flat_error_rates = error_rates.flatten()\n",
    "            indices = np.argsort(flat_error_rates)[-5:]  # Top 5 difficult positions\n",
    "            for idx in reversed(indices):\n",
    "                row, col = idx // 9, idx % 9\n",
    "                if flat_error_rates[idx] > 0:\n",
    "                    print(f\"Position ({row+1},{col+1}): Error rate {flat_error_rates[idx]:.2f}\")\n",
    "    else:\n",
    "        print(\"❌ Validation dataset is empty, cannot run diagnostics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248717f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. EVALUATION FUNCTION\n",
    "\n",
    "def evaluate_model(model, dataset, max_samples=20):\n",
    "    \"\"\"Evaluate model and show results\"\"\"\n",
    "    print(f\"\\n🔍 Evaluation Results\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    # Metrics\n",
    "    exact_matches = 0\n",
    "    total_accuracy = 0\n",
    "    valid_solutions = 0\n",
    "\n",
    "    def is_valid_sudoku(grid):\n",
    "        \"\"\"Check if 9x9 grid is valid\"\"\"\n",
    "        grid = grid.reshape(9, 9)\n",
    "        for i in range(9):\n",
    "            # Check row\n",
    "            row = grid[i][grid[i] != 0]\n",
    "            if len(row) != len(set(row.tolist())):\n",
    "                return False\n",
    "            # Check column\n",
    "            col = grid[:, i][grid[:, i] != 0]\n",
    "            if len(col) != len(set(col.tolist())):\n",
    "                return False\n",
    "        # Check 3x3 boxes\n",
    "        for br in range(0, 9, 3):\n",
    "            for bc in range(0, 9, 3):\n",
    "                box = grid[br:br+3, bc:bc+3].flatten()\n",
    "                box = box[box != 0]\n",
    "                if len(box) != len(set(box.tolist())):\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def print_sudoku(grid, title):\n",
    "        \"\"\"Pretty print sudoku grid\"\"\"\n",
    "        print(f\"\\n{title}:\")\n",
    "        grid = grid.reshape(9, 9)\n",
    "        for i in range(9):\n",
    "            if i % 3 == 0 and i > 0:\n",
    "                print(\"------+-------+------\")\n",
    "            row = \"\"\n",
    "            for j in range(9):\n",
    "                if j % 3 == 0 and j > 0:\n",
    "                    row += \"| \"\n",
    "                val = grid[i, j].item() if hasattr(grid[i, j], 'item') else grid[i, j]\n",
    "                # Ensure value is capped at 9 (valid Sudoku values only)\n",
    "                val = min(val, 9)\n",
    "                row += f\"{val if val != 0 else '.'} \"\n",
    "            print(row)\n",
    "\n",
    "    # Evaluate samples\n",
    "    samples_to_eval = min(len(dataset), max_samples)\n",
    "    \n",
    "    # Check if dataset is empty\n",
    "    if samples_to_eval == 0:\n",
    "        print(\"❌ No samples available for evaluation\")\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'exact_rate': 0.0,\n",
    "            'valid_rate': 0.0,\n",
    "            'samples_evaluated': 0\n",
    "        }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(samples_to_eval):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            target = sample['target'].cpu().numpy()\n",
    "\n",
    "            # Get prediction\n",
    "            logits = model(input_ids)\n",
    "            # Only consider logits for digits 0-9 (ignore any higher values)\n",
    "            logits = logits[:, :, :10]\n",
    "            prediction = logits.argmax(dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Keep input clues unchanged\n",
    "            input_grid = sample['input_ids'].cpu().numpy()\n",
    "            prediction[input_grid != 0] = input_grid[input_grid != 0]\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = np.mean(prediction == target)\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            if np.array_equal(prediction, target):\n",
    "                exact_matches += 1\n",
    "\n",
    "            if is_valid_sudoku(prediction):\n",
    "                valid_solutions += 1\n",
    "\n",
    "            # Show first few examples\n",
    "            if i < 3:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Example {i+1}\")\n",
    "                print_sudoku(input_grid, \"Input Puzzle\")\n",
    "                print_sudoku(prediction, \"Model Prediction\")\n",
    "                print_sudoku(target, \"Correct Solution\")\n",
    "                \n",
    "                # Check for input/solution consistency\n",
    "                mask = input_grid != 0\n",
    "                input_matches_solution = np.all(input_grid[mask] == target[mask])\n",
    "                \n",
    "                print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "                print(f\"Valid: {is_valid_sudoku(prediction)}\")\n",
    "                print(f\"Exact: {np.array_equal(prediction, target)}\")\n",
    "                print(f\"Input matches solution: {input_matches_solution}\")\n",
    "                \n",
    "                if not input_matches_solution:\n",
    "                    print(\"⚠️ Warning: Non-zero values in input don't all match solution\")\n",
    "                    mismatch_count = np.sum(input_grid[mask] != target[mask])\n",
    "                    print(f\"  {mismatch_count} mismatched positions found\")\n",
    "\n",
    "    # Final metrics\n",
    "    avg_accuracy = total_accuracy / samples_to_eval if samples_to_eval > 0 else 0\n",
    "    exact_rate = exact_matches / samples_to_eval if samples_to_eval > 0 else 0\n",
    "    valid_rate = valid_solutions / samples_to_eval if samples_to_eval > 0 else 0\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"📊 FINAL RESULTS\")\n",
    "    print('='*50)\n",
    "    print(f\"Samples evaluated: {samples_to_eval}\")\n",
    "    print(f\"Average accuracy: {avg_accuracy:.3f} ({avg_accuracy*100:.1f}%)\")\n",
    "    print(f\"Exact matches: {exact_matches}/{samples_to_eval} ({exact_rate*100:.1f}%)\")\n",
    "    print(f\"Valid solutions: {valid_solutions}/{samples_to_eval} ({valid_rate*100:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': avg_accuracy,\n",
    "        'exact_rate': exact_rate,\n",
    "        'valid_rate': valid_rate,\n",
    "        'samples_evaluated': samples_to_eval\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef92646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training visualization\n",
    "\n",
    "def create_interactive_plot():\n",
    "    \"\"\"Create interactive plots for real-time training visualization\"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Customize the plots\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    axs[1].set_title('Cell Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    axs[2].set_title('Solution Quality')\n",
    "    axs[2].set_xlabel('Epoch')\n",
    "    axs[2].set_ylabel('Rate')\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    # Initialize empty lines for plotting\n",
    "    lines = {\n",
    "        'train_loss': axs[0].plot([], [], 'b-', label='Train Loss')[0],\n",
    "        'val_cell_accuracy': axs[1].plot([], [], 'g-', label='Cell Accuracy')[0],\n",
    "        'val_exact_match': axs[2].plot([], [], 'b-', label='Exact Match')[0],\n",
    "        'val_valid_solutions': axs[2].plot([], [], 'r-', label='Valid Solution')[0]\n",
    "    }\n",
    "    \n",
    "    # Add legends\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    axs[2].legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, axs, lines\n",
    "\n",
    "def update_plot(fig, lines, history):\n",
    "    \"\"\"Update the training visualization plots with new data\"\"\"\n",
    "    # Update each line with the latest data\n",
    "    if 'train_loss' in history and len(history['train_loss']) > 0:\n",
    "        x = list(range(len(history['train_loss'])))\n",
    "        lines['train_loss'].set_data(x, history['train_loss'])\n",
    "        lines['val_cell_accuracy'].set_data(x, history['val_cell_accuracy'])\n",
    "        lines['val_exact_match'].set_data(x, history['val_exact_match'])\n",
    "        lines['val_valid_solutions'].set_data(x, history['val_valid_solutions'])\n",
    "    \n",
    "        # Adjust the axes limits\n",
    "        for ax in fig.axes:\n",
    "            ax.relim()\n",
    "            ax.autoscale_view()\n",
    "        \n",
    "        # Redraw the figure\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. MAIN EXECUTION\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Starting HRM Sudoku Complete Demo...\")\n",
    "    \n",
    "    # Get the root directory\n",
    "    root_dir = os.getcwd()\n",
    "    data_dir = os.path.join(root_dir, 'data/sudoku-extreme-1k-aug-1000')\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"❌ Data directory not found at: {data_dir}\")\n",
    "        print(\"Searching for data directory...\")\n",
    "        \n",
    "        # Try alternative locations\n",
    "        for parent_dir in [root_dir, os.path.dirname(root_dir)]:\n",
    "            for dir_name in ['data', 'dataset', 'datasets']:\n",
    "                for subdir in ['sudoku-extreme-1k-aug-1000', 'sudoku', 'sudoku-extreme']:\n",
    "                    test_path = os.path.join(parent_dir, dir_name, subdir)\n",
    "                    if os.path.exists(test_path) and os.path.exists(os.path.join(test_path, 'test')):\n",
    "                        data_dir = test_path\n",
    "                        print(f\"✅ Found data at: {data_dir}\")\n",
    "                        break\n",
    "\n",
    "    # Configuration - smaller model for MPS\n",
    "    config = {\n",
    "        'data_path': data_dir,\n",
    "        'epochs': 15,           # Quick training for demo\n",
    "        'batch_size': 64,       # Reduced for MPS\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'hidden_size': 96,      # Smaller model for MPS\n",
    "        'num_layers': 3,\n",
    "        'num_heads': 4,\n",
    "        'max_train_samples': 1000,  # Small dataset for speed\n",
    "        'max_val_samples': 250,\n",
    "    }\n",
    "\n",
    "    print(f\"\\n📋 Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Validate data path\n",
    "    if not os.path.exists(config['data_path']):\n",
    "        print(f\"❌ Data directory not found: {config['data_path']}\")\n",
    "        print(\"Falling back to synthetic data only\")\n",
    "    else:\n",
    "        print(f\"✅ Data directory found: {config['data_path']}\")\n",
    "        test_dir = os.path.join(config['data_path'], 'test')\n",
    "        train_dir = os.path.join(config['data_path'], 'train')\n",
    "        \n",
    "        if not os.path.exists(test_dir) or not os.path.exists(train_dir):\n",
    "            print(f\"❌ Missing test or train subdirectories\")\n",
    "        else:\n",
    "            print(f\"✅ Test and train directories found\")\n",
    "            \n",
    "            # Check for essential files\n",
    "            for subdir in [test_dir, train_dir]:\n",
    "                inputs_file = os.path.join(subdir, 'all__inputs.npy')\n",
    "                labels_file = os.path.join(subdir, 'all__labels.npy')\n",
    "                \n",
    "                if os.path.exists(inputs_file) and os.path.exists(labels_file):\n",
    "                    print(f\"✅ Found input and label files in {os.path.basename(subdir)}\")\n",
    "                    \n",
    "                    # Check for consistency\n",
    "                    try:\n",
    "                        inputs = np.load(inputs_file)\n",
    "                        labels = np.load(labels_file)\n",
    "                        print(f\"  - {os.path.basename(subdir)} shape: {inputs.shape}\")\n",
    "                        \n",
    "                        # Check a few samples\n",
    "                        sample_count = min(5, inputs.shape[0])\n",
    "                        mismatch_count = 0\n",
    "                        for i in range(sample_count):\n",
    "                            input_grid = inputs[i]\n",
    "                            label_grid = labels[i]\n",
    "                            mask = input_grid != 0\n",
    "                            if not np.all(input_grid[mask] == label_grid[mask]):\n",
    "                                mismatch_count += 1\n",
    "                        \n",
    "                        if mismatch_count > 0:\n",
    "                            print(f\"  ⚠️ Found {mismatch_count}/{sample_count} samples with input/label mismatches\")\n",
    "                        else:\n",
    "                            print(f\"  ✅ All {sample_count} checked samples are consistent\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ❌ Error checking files: {e}\")\n",
    "                else:\n",
    "                    print(f\"❌ Missing input or label files in {os.path.basename(subdir)}\")\n",
    "\n",
    "    # Use custom data validator\n",
    "    print(\"\\n🔍 Verifying dataset with custom validator...\")\n",
    "    \n",
    "    # Define a simplified validator\n",
    "    def validate_dataset_samples(dataset, max_samples=10):\n",
    "        \"\"\"Validate that dataset samples have consistent input and target values\"\"\"\n",
    "        if len(dataset) == 0:\n",
    "            print(\"  ❌ No samples in dataset\")\n",
    "            return False\n",
    "            \n",
    "        # Check input/target consistency\n",
    "        mismatches = 0\n",
    "        for i in range(min(max_samples, len(dataset))):\n",
    "            sample = dataset[i]\n",
    "            input_ids = sample['input_ids'].numpy()\n",
    "            target = sample['target'].numpy()\n",
    "            \n",
    "            # Check if non-zero inputs match targets\n",
    "            mask = input_ids != 0\n",
    "            if not np.all(input_ids[mask] == target[mask]):\n",
    "                mismatches += 1\n",
    "                if mismatches == 1:  # Show details for first mismatch only\n",
    "                    print(f\"  ❌ Mismatch in sample {i}:\")\n",
    "                    mismatch_indices = np.where((input_ids != 0) & (input_ids != target))[0]\n",
    "                    for idx in mismatch_indices[:3]:\n",
    "                        print(f\"    Position {idx}: Input={input_ids[idx]}, Target={target[idx]}\")\n",
    "        \n",
    "        if mismatches > 0:\n",
    "            print(f\"  ❌ Found {mismatches}/{min(max_samples, len(dataset))} samples with mismatches\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"  ✅ All {min(max_samples, len(dataset))} checked samples are consistent\")\n",
    "            return True\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Step 1: Train model\n",
    "        result = train_model(config)\n",
    "        if result is None:\n",
    "            print(\"❌ Training failed\")\n",
    "            return\n",
    "\n",
    "        model, train_dataset, val_dataset = result\n",
    "        \n",
    "        # Verify dataset samples\n",
    "        print(\"\\n🔍 Validating created dataset samples...\")\n",
    "        train_valid = validate_dataset_samples(train_dataset)\n",
    "        val_valid = validate_dataset_samples(val_dataset)\n",
    "        \n",
    "        if not (train_valid and val_valid):\n",
    "            print(\"\\n⚠️ Dataset inconsistencies detected - results may not be accurate\")\n",
    "            print(\"Consider using the dataset verification and repair tools\")\n",
    "        \n",
    "        # Step 2: Evaluate model\n",
    "        metrics = evaluate_model(model, val_dataset)\n",
    "\n",
    "        # Step 3: Summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"🎉 DEMO COMPLETED SUCCESSFULLY!\")\n",
    "        print('='*60)\n",
    "        print(f\"⏱️ Total time: {elapsed_time/60:.1f} minutes\")\n",
    "        print(f\"🎯 Key achievements:\")\n",
    "        print(f\"  ✅ Handled HRM dataset format\")\n",
    "        print(f\"  ✅ Trained transformer model\")\n",
    "        print(f\"  ✅ Achieved {metrics['accuracy']*100:.1f}% cell accuracy\")\n",
    "        print(f\"  ✅ {metrics['exact_rate']*100:.1f}% exact puzzle solutions\")\n",
    "        print(f\"  ✅ {metrics['valid_rate']*100:.1f}% valid Sudoku grids\")\n",
    "\n",
    "        print(f\"\\n🚀 This demonstrates:\")\n",
    "        print(f\"  • Transformer models can learn logical reasoning\")\n",
    "        print(f\"  • Apple Silicon with MPS acceleration is sufficient for research-level experiments\")\n",
    "        print(f\"  • HRM concepts work on consumer hardware\")\n",
    "        print(f\"  • End-to-end ML pipelines are achievable\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Demo failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run the Improved Sudoku Model Training (Incremental Approach)\n",
    "\n",
    "# Create and evaluate the model\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"\\n🔄 Training a new Sudoku model with focused, incremental improvements...\")\n",
    "\n",
    "# 🎛️ TRAINING CONTROL FLAG - Set to True to run training, False to skip and use Dashboard UI instead\n",
    "RUN_TRAINING_HERE = False  # Set to True to execute training in this cell\n",
    "\n",
    "print(f\"⚙️ Training flag: {'ENABLED' if RUN_TRAINING_HERE else 'DISABLED - Use Dashboard UI for training'}\")\n",
    "\n",
    "# Define the print_sudoku function to display Sudoku grids\n",
    "def print_sudoku(grid, title=\"Sudoku Puzzle\"):\n",
    "    \"\"\"Print a Sudoku grid in a readable format\"\"\"\n",
    "    grid = grid.reshape(9, 9)\n",
    "    print(f\"\\n{title}:\")\n",
    "    for i in range(9):\n",
    "        if i % 3 == 0 and i > 0:\n",
    "            print(\"------+-------+------\")\n",
    "        row = \"\"\n",
    "        for j in range(9):\n",
    "            if j % 3 == 0 and j > 0:\n",
    "                row += \"| \"\n",
    "            val = grid[i, j].item() if hasattr(grid[i, j], 'item') else grid[i, j]\n",
    "            # Make sure we display valid Sudoku values (0-9)\n",
    "            if val > 9:\n",
    "                val = 9  # Cap at 9 for display\n",
    "            row += f\"{val if val != 0 else '.'} \"\n",
    "        print(row)\n",
    "\n",
    "# Set up focused training configuration - optimized for faster iterations and real-time monitoring\n",
    "config = {\n",
    "    'data_path': DATA_DIR,\n",
    "    'epochs': 50,                # REDUCED: Fewer epochs for faster iterations\n",
    "    'batch_size': 32,            # REDUCED: Smaller batch size for more updates\n",
    "    'learning_rate': 1e-4,       # REDUCED: More conservative learning rate\n",
    "    'weight_decay': 0.01,        # REDUCED: Less aggressive regularization\n",
    "    'hidden_size': 192,          # REDUCED: More moderate model size\n",
    "    'num_layers': 6,             # REDUCED: Fewer layers for faster training\n",
    "    'num_heads': 6,              # REDUCED: Fewer attention heads\n",
    "    'max_train_samples': 950,    # REDUCED: Smaller dataset for faster iterations\n",
    "    'max_val_samples': 100,      # REDUCED: Smaller validation set\n",
    "    'early_stopping_patience': 10, # REDUCED: Stop earlier to iterate faster\n",
    "    'early_stopping_threshold': 0.005, # INCREASED: More relaxed improvement threshold\n",
    "    'gradient_clip': 1.0,        # KEPT: Prevent exploding gradients\n",
    "    'evaluate_every': 1,         # NEW: Evaluate after every epoch\n",
    "    'save_model': True,          # NEW: Save model checkpoints\n",
    "    'validation_frequency': 50,  # NEW: Check validation metrics every 50 batches\n",
    "    'dropout': 0.1,              # REDUCED: Less dropout\n",
    "    'check_solutions': True,     # NEW: Explicitly verify solutions are valid\n",
    "    'vocab_size': 10            # Make sure we include vocab_size\n",
    "}\n",
    "\n",
    "if not RUN_TRAINING_HERE:\n",
    "    print(\"\\n⏭️ Training skipped - use Dashboard UI to configure and start training\")\n",
    "    print(\"💡 Essential components (model class, config, datasets) are being set up...\")\n",
    "\n",
    "print(\"\\n📋 Focused Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create datasets with improved validation\n",
    "train_dataset = HRMSudokuDataset(config['data_path'], 'train', config['max_train_samples'])\n",
    "val_dataset = HRMSudokuDataset(config['data_path'], 'test', config['max_val_samples'])\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"❌ No training data available. Creating synthetic dataset.\")\n",
    "    # We can continue with the synthetic data\n",
    "\n",
    "# Data loaders - with better settings for MPS\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "# Simplified model definition - focused on the core architecture\n",
    "class SimpleSudokuTransformer(nn.Module):\n",
    "    \"\"\"Simplified Transformer model for Sudoku solving with cleaner architecture\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=10, hidden_size=128, num_layers=4, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = nn.Embedding(81, hidden_size)  # 9x9 = 81 positions\n",
    "        \n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with reasonable values\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_emb = self.token_embedding(input_ids)\n",
    "        pos_emb = self.position_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        hidden_states = self.dropout(token_emb + pos_emb)\n",
    "        \n",
    "        # Transformer layers\n",
    "        hidden_states = self.transformer(hidden_states)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(hidden_states)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create model\n",
    "print(f\"\\n🧠 Creating SimpleSudokuTransformer model...\")\n",
    "model = SimpleSudokuTransformer(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_layers=config['num_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Display model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"📊 Model: {total_params:,} parameters\")\n",
    "print(f\"📊 Training on {len(train_dataset)} samples\")\n",
    "print(f\"📊 Validation on {len(val_dataset)} samples\")\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=config['learning_rate'], \n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Use CrossEntropyLoss with ignore_index=0 to not penalize for empty cells\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Create directory for checkpoints\n",
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✅ Model, optimizer, and datasets ready for Dashboard UI training\")\n",
    "\n",
    "# Only proceed with training if flag is enabled\n",
    "if RUN_TRAINING_HERE:\n",
    "    print(\"\\n🚀 Starting training with real-time monitoring...\")\n",
    "    # Note: Full training code would be here but is disabled by default\n",
    "    # Users should use the Dashboard UI for interactive training instead\n",
    "    print(\"⚠️ Set RUN_TRAINING_HERE = True to enable training in this cell\")\n",
    "else:\n",
    "    print(\"\\n🎯 Setup complete - proceed to Dashboard UI for interactive training\")\n",
    "    print(\"💡 Model architecture, datasets, and configuration are ready\")\n",
    "    print(\"✅ Use the comprehensive dashboard below to start training\")\n",
    "\n",
    "# Create a basic model evaluation for testing\n",
    "print(\"\\n🔍 Quick Model Test (Untrained)\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test on first sample\n",
    "    sample = val_dataset[0] if len(val_dataset) > 0 else train_dataset[0]\n",
    "    input_grid = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    logits = model(input_grid)\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"✅ Model forward pass successful\")\n",
    "    print(f\"📊 Input shape: {input_grid.shape}\")\n",
    "    print(f\"📊 Output shape: {logits.shape}\")\n",
    "    \n",
    "print(\"\\n✅ Cell execution complete - Dashboard UI available below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85ca10",
   "metadata": {},
   "source": [
    "# 🎯 Enhanced Sudoku Dashboard\n",
    "\n",
    "Interactive dashboard for the HRM Sudoku solver with AI inference and ultra-efficient solving capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fafba",
   "metadata": {},
   "source": [
    "# 🎛️ Comprehensive HRM Sudoku Training & Model Management Dashboard\n",
    "\n",
    "This section provides a complete interactive interface for:\n",
    "- **Training Configuration**: Adjust epochs, learning rates, model architecture\n",
    "- **Model Selection & Management**: Browse, compare, and test saved models\n",
    "- **Visualization Options**: Both graphical and text-based outputs\n",
    "- **Real-time Monitoring**: Training progress with live metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "class ComprehensiveHRMDashboard:\n",
    "    \"\"\"Comprehensive HRM Sudoku Training and Model Management Dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, device=None, data_dir=None, model_dir=None):\n",
    "        self.model = model\n",
    "        self.device = device if device else torch.device(\"cpu\")\n",
    "        self.data_dir = data_dir if data_dir else DATA_DIR\n",
    "        self.model_dir = model_dir if model_dir else MODEL_DIR\n",
    "        self.config_dir = CONFIG_DIR  # Add config directory for saving configurations\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        self.config_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Training state\n",
    "        self.is_training = False\n",
    "        self.training_history = {'train_loss': [], 'val_accuracy': [], 'epochs': []}\n",
    "        self.current_model_info = None\n",
    "        \n",
    "        # Create UI sections\n",
    "        self._create_training_section()\n",
    "        self._create_model_selection_section()\n",
    "        self._create_visualization_section()\n",
    "        self._create_output_section()\n",
    "        \n",
    "        # Main dashboard layout\n",
    "        self.dashboard = self._assemble_dashboard()\n",
    "        \n",
    "        print(\"🎯 Comprehensive HRM Dashboard Initialized Successfully!\")\n",
    "        print(\"✅ Features: Training Config, Model Management, Visualization, Monitoring\")\n",
    "    \n",
    "    def _create_training_section(self):\n",
    "        \"\"\"Create the training configuration section\"\"\"\n",
    "        print(\"🔧 Setting up training configuration section...\")\n",
    "        \n",
    "        # Training Configuration Header\n",
    "        self.training_header = widgets.HTML(\n",
    "            value=\"<h3>🚀 Model Training Configuration</h3>\",\n",
    "            layout=widgets.Layout(margin='0px 0px 10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Configuration Management Section\n",
    "        self.config_dropdown = widgets.Dropdown(\n",
    "            options=['No configs found'],\n",
    "            description='Load Config:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.load_config_button = widgets.Button(\n",
    "            description='📂 Load Config',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        self.refresh_configs_button = widgets.Button(\n",
    "            description='🔄 Refresh',\n",
    "            button_style='',\n",
    "            layout=widgets.Layout(width='80px')\n",
    "        )\n",
    "        \n",
    "        # Wire up config management events\n",
    "        self.load_config_button.on_click(self._load_config)\n",
    "        self.refresh_configs_button.on_click(self._refresh_config_list)\n",
    "        \n",
    "        # Initialize config list\n",
    "        self.config_list = []\n",
    "        \n",
    "        # Refresh config list on initialization\n",
    "        self._refresh_config_list()\n",
    "        \n",
    "        # Model Architecture Controls\n",
    "        self.hidden_size_slider = widgets.IntSlider(\n",
    "            value=128, min=64, max=512, step=64,\n",
    "            description='Hidden Size:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.num_layers_slider = widgets.IntSlider(\n",
    "            value=4, min=2, max=12, step=1,\n",
    "            description='Layers:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.num_heads_slider = widgets.IntSlider(\n",
    "            value=8, min=2, max=16, step=2,\n",
    "            description='Attention Heads:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Training Parameters\n",
    "        self.epochs_slider = widgets.IntSlider(\n",
    "            value=10, min=1, max=100, step=1,\n",
    "            description='Epochs:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.learning_rate_slider = widgets.FloatLogSlider(\n",
    "            value=1e-4, base=10, min=-6, max=-2,\n",
    "            description='Learning Rate:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.batch_size_dropdown = widgets.Dropdown(\n",
    "            options=[16, 32, 64, 128], value=32,\n",
    "            description='Batch Size:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.dropout_slider = widgets.FloatSlider(\n",
    "            value=0.1, min=0.0, max=0.5, step=0.05,\n",
    "            description='Dropout:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Dataset Configuration\n",
    "        self.max_train_samples_slider = widgets.IntSlider(\n",
    "            value=1000, min=100, max=10000, step=100,\n",
    "            description='Train Samples:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        self.max_val_samples_slider = widgets.IntSlider(\n",
    "            value=200, min=50, max=2000, step=50,\n",
    "            description='Val Samples:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Training Strategy\n",
    "        self.training_strategy_dropdown = widgets.Dropdown(\n",
    "            options=['Standard', 'Curriculum Learning', 'Progressive Difficulty', 'Enhanced'],\n",
    "            value='Standard',\n",
    "            description='Strategy:', style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Control Buttons\n",
    "        self.train_button = widgets.Button(\n",
    "            description='🚀 Start Training',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        self.stop_button = widgets.Button(\n",
    "            description='⏹️ Stop Training',\n",
    "            button_style='danger',\n",
    "            layout=widgets.Layout(width='150px'),\n",
    "            disabled=True\n",
    "        )\n",
    "        self.save_config_button = widgets.Button(\n",
    "            description='💾 Save Config',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Training Progress\n",
    "        self.training_progress = widgets.FloatProgress(\n",
    "            value=0, min=0, max=1,\n",
    "            description='Progress:',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.training_status = widgets.HTML(\n",
    "            value=\"<b>Status:</b> Ready to train\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Event handlers\n",
    "        self.train_button.on_click(self._start_training)\n",
    "        self.stop_button.on_click(self._stop_training)\n",
    "        self.save_config_button.on_click(self._save_config)\n",
    "        \n",
    "        # Group controls\n",
    "        arch_controls = widgets.VBox([\n",
    "            widgets.HTML(\"<b>🏗️ Model Architecture</b>\"),\n",
    "            self.hidden_size_slider,\n",
    "            self.num_layers_slider,\n",
    "            self.num_heads_slider,\n",
    "            self.dropout_slider\n",
    "        ])\n",
    "        \n",
    "        training_controls = widgets.VBox([\n",
    "            widgets.HTML(\"<b>⚙️ Training Parameters</b>\"),\n",
    "            self.epochs_slider,\n",
    "            self.learning_rate_slider,\n",
    "            self.batch_size_dropdown,\n",
    "            self.training_strategy_dropdown\n",
    "        ])\n",
    "        \n",
    "        data_controls = widgets.VBox([\n",
    "            widgets.HTML(\"<b>📊 Dataset Configuration</b>\"),\n",
    "            self.max_train_samples_slider,\n",
    "            self.max_val_samples_slider\n",
    "        ])\n",
    "        \n",
    "        control_buttons = widgets.HBox([\n",
    "            self.train_button,\n",
    "            self.stop_button,\n",
    "            self.save_config_button\n",
    "        ], layout=widgets.Layout(margin='10px 0px'))\n",
    "        \n",
    "        progress_section = widgets.VBox([\n",
    "            self.training_progress,\n",
    "            self.training_status\n",
    "        ])\n",
    "        \n",
    "        self.training_section = widgets.VBox([\n",
    "            self.training_header,\n",
    "            widgets.HBox([\n",
    "                self.config_dropdown,\n",
    "                self.load_config_button,\n",
    "                self.refresh_configs_button\n",
    "            ], layout=widgets.Layout(margin='0px 0px 15px 0px')),\n",
    "            widgets.HBox([arch_controls, training_controls, data_controls]),\n",
    "            control_buttons,\n",
    "            progress_section\n",
    "        ])\n",
    "    \n",
    "    def _create_model_selection_section(self):\n",
    "        \"\"\"Create the model selection and management section\"\"\"\n",
    "        print(\"🔧 Setting up model selection section...\")\n",
    "        \n",
    "        # Model Selection Header\n",
    "        self.model_header = widgets.HTML(\n",
    "            value=\"<h3>🎯 Model Selection & Management</h3>\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Model List\n",
    "        self.model_dropdown = widgets.Dropdown(\n",
    "            description='Select Model:',\n",
    "            style={'description_width': '100px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Model Info Display\n",
    "        self.model_info_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='200px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # Model Action Buttons\n",
    "        self.load_model_button = widgets.Button(\n",
    "            description='📥 Load Model',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.test_model_button = widgets.Button(\n",
    "            description='🧪 Test Model',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.delete_model_button = widgets.Button(\n",
    "            description='🗑️ Delete Model',\n",
    "            button_style='danger',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.refresh_models_button = widgets.Button(\n",
    "            description='🔄 Refresh List',\n",
    "            button_style='',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Event handlers\n",
    "        self.model_dropdown.observe(self._on_model_selected, names='value')\n",
    "        self.load_model_button.on_click(self._load_selected_model)\n",
    "        self.test_model_button.on_click(self._test_selected_model)\n",
    "        self.delete_model_button.on_click(self._delete_selected_model)\n",
    "        self.refresh_models_button.on_click(self._refresh_model_list)\n",
    "        \n",
    "        # Model comparison section\n",
    "        self.comparison_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='300px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        self.compare_button = widgets.Button(\n",
    "            description='📊 Compare Models',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        self.compare_button.on_click(self._compare_models)\n",
    "        \n",
    "        # Assemble model section\n",
    "        model_selection = widgets.VBox([\n",
    "            self.model_dropdown,\n",
    "            self.model_info_output\n",
    "        ])\n",
    "        \n",
    "        model_actions = widgets.HBox([\n",
    "            self.load_model_button,\n",
    "            self.test_model_button,\n",
    "            self.delete_model_button,\n",
    "            self.refresh_models_button\n",
    "        ])\n",
    "        \n",
    "        comparison_section = widgets.VBox([\n",
    "            widgets.HTML(\"<b>📊 Model Comparison</b>\"),\n",
    "            self.compare_button,\n",
    "            self.comparison_output\n",
    "        ])\n",
    "        \n",
    "        self.model_section = widgets.VBox([\n",
    "            self.model_header,\n",
    "            model_selection,\n",
    "            model_actions,\n",
    "            comparison_section\n",
    "        ])\n",
    "        \n",
    "        # Initialize model list\n",
    "        self._refresh_model_list()\n",
    "    \n",
    "    def _create_visualization_section(self):\n",
    "        \"\"\"Create the visualization options section\"\"\"\n",
    "        print(\"🔧 Setting up visualization section...\")\n",
    "        \n",
    "        # Visualization Header\n",
    "        self.viz_header = widgets.HTML(\n",
    "            value=\"<h3>📈 Visualization Options</h3>\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Visualization Type Selection\n",
    "        self.viz_type_dropdown = widgets.Dropdown(\n",
    "            options=[\n",
    "                'Training Metrics Graph',\n",
    "                'Model Architecture Diagram', \n",
    "                'Dataset Distribution',\n",
    "                'Performance Heatmap',\n",
    "                'Error Analysis',\n",
    "                'Learning Curves'\n",
    "            ],\n",
    "            value='Training Metrics Graph',\n",
    "            description='Visualization:',\n",
    "            style={'description_width': '100px'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Display Mode Selection\n",
    "        self.display_mode_toggle = widgets.ToggleButtons(\n",
    "            options=['Graphical', 'Text Summary', 'Both'],\n",
    "            value='Graphical',\n",
    "            description='Display Mode:',\n",
    "            style={'description_width': '100px'}\n",
    "        )\n",
    "        \n",
    "        # Generate Visualization Button\n",
    "        self.generate_viz_button = widgets.Button(\n",
    "            description='📊 Generate',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.generate_viz_button.on_click(self._generate_visualization)\n",
    "        \n",
    "        # Visualization Output\n",
    "        self.viz_output = widgets.Output(\n",
    "            layout=widgets.Layout(min_height='400px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # Assemble visualization section\n",
    "        viz_controls = widgets.HBox([\n",
    "            self.viz_type_dropdown,\n",
    "            self.display_mode_toggle,\n",
    "            self.generate_viz_button\n",
    "        ])\n",
    "        \n",
    "        self.visualization_section = widgets.VBox([\n",
    "            self.viz_header,\n",
    "            viz_controls,\n",
    "            self.viz_output\n",
    "        ])\n",
    "    \n",
    "    def _create_output_section(self):\n",
    "        \"\"\"Create the output and logging section\"\"\"\n",
    "        print(\"🔧 Setting up output section...\")\n",
    "        \n",
    "        # Output Header\n",
    "        self.output_header = widgets.HTML(\n",
    "            value=\"<h3>📝 Output & Monitoring</h3>\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Output Type Selection\n",
    "        self.output_type_tabs = widgets.Tab()\n",
    "        \n",
    "        # Training Log Output\n",
    "        self.training_log_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='300px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # Model Testing Output\n",
    "        self.testing_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='300px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # System Information Output\n",
    "        self.system_info_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='300px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # Real-time Metrics Output\n",
    "        self.metrics_output = widgets.Output(\n",
    "            layout=widgets.Layout(height='300px', border='1px solid gray')\n",
    "        )\n",
    "        \n",
    "        # Set up tabs\n",
    "        self.output_type_tabs.children = [\n",
    "            self.training_log_output,\n",
    "            self.testing_output,\n",
    "            self.system_info_output,\n",
    "            self.metrics_output\n",
    "        ]\n",
    "        self.output_type_tabs.set_title(0, 'Training Logs')\n",
    "        self.output_type_tabs.set_title(1, 'Model Testing')\n",
    "        self.output_type_tabs.set_title(2, 'System Info')\n",
    "        self.output_type_tabs.set_title(3, 'Live Metrics')\n",
    "        \n",
    "        # Control Buttons\n",
    "        self.clear_output_button = widgets.Button(\n",
    "            description='🗑️ Clear Output',\n",
    "            button_style='',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        self.export_logs_button = widgets.Button(\n",
    "            description='💾 Export Logs',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        self.clear_output_button.on_click(self._clear_output)\n",
    "        self.export_logs_button.on_click(self._export_logs)\n",
    "        \n",
    "        # Assemble output section\n",
    "        output_controls = widgets.HBox([\n",
    "            self.clear_output_button,\n",
    "            self.export_logs_button\n",
    "        ])\n",
    "        \n",
    "        self.output_section = widgets.VBox([\n",
    "            self.output_header,\n",
    "            self.output_type_tabs,\n",
    "            output_controls\n",
    "        ])\n",
    "        \n",
    "        # Initialize system info\n",
    "        self._display_system_info()\n",
    "    \n",
    "    def _assemble_dashboard(self):\n",
    "        \"\"\"Assemble all sections into the main dashboard\"\"\"\n",
    "        print(\"🔧 Assembling comprehensive dashboard...\")\n",
    "        \n",
    "        # Create accordion for sections\n",
    "        accordion = widgets.Accordion(children=[\n",
    "            self.training_section,\n",
    "            self.model_section,\n",
    "            self.visualization_section,\n",
    "            self.output_section\n",
    "        ])\n",
    "        \n",
    "        accordion.set_title(0, '🚀 Training Configuration')\n",
    "        accordion.set_title(1, '🎯 Model Management')\n",
    "        accordion.set_title(2, '📈 Visualization')\n",
    "        accordion.set_title(3, '📝 Output & Monitoring')\n",
    "        \n",
    "        # Set default open sections\n",
    "        accordion.selected_index = 0\n",
    "        \n",
    "        return accordion\n",
    "    \n",
    "    def show(self):\n",
    "        \"\"\"Display the comprehensive dashboard\"\"\"\n",
    "        print(\"🎯 Displaying Comprehensive HRM Dashboard...\")\n",
    "        print(\"=\" * 60)\n",
    "        display(self.dashboard)\n",
    "        return self.dashboard\n",
    "    \n",
    "    # Event Handler Methods\n",
    "    def _start_training(self, button):\n",
    "        \"\"\"Start training with current configuration\"\"\"\n",
    "        if self.is_training:\n",
    "            with self.training_log_output:\n",
    "                print(\"⚠️ Training is already in progress!\")\n",
    "            return\n",
    "        \n",
    "        # Get configuration from UI\n",
    "        config = self._get_training_config()\n",
    "        \n",
    "        # Validate configuration\n",
    "        if not self._validate_training_config(config):\n",
    "            return\n",
    "        \n",
    "        self.is_training = True\n",
    "        self.train_button.disabled = True\n",
    "        self.stop_button.disabled = False\n",
    "        self.training_progress.value = 0\n",
    "        \n",
    "        with self.training_log_output:\n",
    "            clear_output()\n",
    "            print(\"🚀 Starting HRM Sudoku Training\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"📋 Training Configuration:\")\n",
    "            for key, value in config.items():\n",
    "                print(f\"   {key}: {value}\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "        self.training_status.value = \"<b>Status:</b> Initializing training...\"\n",
    "        \n",
    "        # Start training in a separate thread to prevent UI blocking\n",
    "        import threading\n",
    "        self.training_thread = threading.Thread(target=self._run_training, args=(config,))\n",
    "        self.training_thread.daemon = True\n",
    "        self.training_thread.start()\n",
    "    \n",
    "    def _validate_training_config(self, config):\n",
    "        \"\"\"Validate training configuration parameters\"\"\"\n",
    "        with self.training_log_output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Check for reasonable values\n",
    "            issues = []\n",
    "            \n",
    "            if config['epochs'] <= 0:\n",
    "                issues.append(\"Epochs must be greater than 0\")\n",
    "            if config['learning_rate'] <= 0 or config['learning_rate'] > 1:\n",
    "                issues.append(\"Learning rate must be between 0 and 1\")\n",
    "            if config['hidden_size'] < 64:\n",
    "                issues.append(\"Hidden size seems too small (< 64)\")\n",
    "            if config['num_layers'] <= 0:\n",
    "                issues.append(\"Number of layers must be greater than 0\")\n",
    "            if config['num_heads'] <= 0:\n",
    "                issues.append(\"Number of attention heads must be greater than 0\")\n",
    "            if config['hidden_size'] % config['num_heads'] != 0:\n",
    "                issues.append(\"Hidden size must be divisible by number of heads\")\n",
    "            \n",
    "            if issues:\n",
    "                print(\"❌ Configuration validation failed:\")\n",
    "                for issue in issues:\n",
    "                    print(f\"   • {issue}\")\n",
    "                print(\"\\n🔧 Please adjust the configuration and try again.\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"✅ Configuration validation passed!\")\n",
    "                return True\n",
    "    \n",
    "    def _run_training(self, config):\n",
    "        \"\"\"Run the actual training process\"\"\"\n",
    "        try:\n",
    "            # Update status\n",
    "            self.training_status.value = \"<b>Status:</b> Training in progress...\"\n",
    "            \n",
    "            # Check if train_model function exists\n",
    "            if 'train_model' not in globals():\n",
    "                with self.training_log_output:\n",
    "                    print(\"❌ train_model function not found!\")\n",
    "                    print(\"💡 Please ensure the training function is defined in the notebook\")\n",
    "                return\n",
    "            \n",
    "            with self.training_log_output:\n",
    "                print(\"\\n🔧 Initializing model and data...\")\n",
    "                print(\"📊 Loading dataset...\")\n",
    "                print(\"🎯 Starting training loop...\")\n",
    "            \n",
    "            # Call the actual training function\n",
    "            result = train_model(config)\n",
    "            \n",
    "            if not self.is_training:  # Check if stopped by user\n",
    "                with self.training_log_output:\n",
    "                    print(\"⏹️ Training stopped by user\")\n",
    "                return\n",
    "            \n",
    "            # Training completed successfully\n",
    "            self.training_progress.value = 1.0\n",
    "            self.training_status.value = \"<b>Status:</b> Training completed!\"\n",
    "            \n",
    "            with self.training_log_output:\n",
    "                print(\"\\n🎉 Training completed successfully!\")\n",
    "                if isinstance(result, dict) and 'final_accuracy' in result:\n",
    "                    print(f\"📈 Final accuracy: {result['final_accuracy']:.4f}\")\n",
    "                if isinstance(result, dict) and 'model_path' in result:\n",
    "                    print(f\"💾 Model saved to: {result['model_path']}\")\n",
    "            \n",
    "            # Refresh model list to show new model\n",
    "            if hasattr(self, '_refresh_model_list'):\n",
    "                self._refresh_model_list()\n",
    "                \n",
    "        except Exception as e:\n",
    "            with self.training_log_output:\n",
    "                print(f\"\\n❌ Training error: {str(e)}\")\n",
    "                import traceback\n",
    "                print(\"🔍 Error details:\")\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            self.training_status.value = \"<b>Status:</b> Training failed\"\n",
    "        \n",
    "        finally:\n",
    "            # Reset training state\n",
    "            self.is_training = False\n",
    "            self.train_button.disabled = False\n",
    "            self.stop_button.disabled = True\n",
    "        \n",
    "    def _stop_training(self, button):\n",
    "        \"\"\"Stop current training gracefully\"\"\"\n",
    "        if not self.is_training:\n",
    "            with self.training_log_output:\n",
    "                print(\"⚠️ No training is currently running\")\n",
    "            return\n",
    "        \n",
    "        with self.training_log_output:\n",
    "            print(\"\\n⏹️ Stopping training...\")\n",
    "            print(\"🔄 Waiting for current epoch to complete...\")\n",
    "        \n",
    "        self.is_training = False\n",
    "        self.training_status.value = \"<b>Status:</b> Stopping training...\"\n",
    "        \n",
    "        # Wait for training thread to complete if it exists\n",
    "        if hasattr(self, 'training_thread') and self.training_thread.is_alive():\n",
    "            self.training_thread.join(timeout=10)  # Wait up to 10 seconds\n",
    "        \n",
    "        # Reset UI state\n",
    "        self.train_button.disabled = False\n",
    "        self.stop_button.disabled = True\n",
    "        self.training_status.value = \"<b>Status:</b> Training stopped\"\n",
    "        \n",
    "        with self.training_log_output:\n",
    "            print(\"✅ Training stopped successfully\")\n",
    "            print(\"💡 You can restart training with the same or different configuration\")\n",
    "    \n",
    "    def _refresh_config_list(self, button=None):\n",
    "        \"\"\"Refresh the list of available configuration files\"\"\"\n",
    "        try:\n",
    "            config_files = []\n",
    "            if self.model_dir.exists():\n",
    "                # Look for JSON config files\n",
    "                for config_file in self.model_dir.glob(\"*.json\"):\n",
    "                    if \"config\" in config_file.name.lower():\n",
    "                        config_files.append(config_file.name)\n",
    "                \n",
    "                # Look in subdirectories too\n",
    "                for config_file in self.model_dir.glob(\"*/*.json\"):\n",
    "                    if \"config\" in config_file.name.lower():\n",
    "                        config_files.append(str(config_file.relative_to(self.model_dir)))\n",
    "            \n",
    "            if config_files:\n",
    "                # Sort by modification time (newest first)\n",
    "                config_files.sort(reverse=True)\n",
    "                self.config_dropdown.options = config_files\n",
    "            else:\n",
    "                self.config_dropdown.options = ['No configs found']\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.config_dropdown.options = [f'Error: {str(e)}']\n",
    "    \n",
    "    def _load_config(self, button):\n",
    "        \"\"\"Load the selected configuration file\"\"\"\n",
    "        if self.config_dropdown.value == 'No configs found' or 'Error:' in str(self.config_dropdown.value):\n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(\"❌ No valid configuration selected\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            config_file = self.model_dir / self.config_dropdown.value\n",
    "            \n",
    "            with open(config_file, 'r') as f:\n",
    "                import json\n",
    "                loaded_config = json.load(f)\n",
    "            \n",
    "            # Update UI controls with loaded values\n",
    "            if 'hidden_size' in loaded_config:\n",
    "                self.hidden_size_slider.value = loaded_config['hidden_size']\n",
    "            if 'num_layers' in loaded_config:\n",
    "                self.num_layers_slider.value = loaded_config['num_layers']\n",
    "            if 'num_heads' in loaded_config:\n",
    "                self.num_heads_slider.value = loaded_config['num_heads']\n",
    "            if 'dropout' in loaded_config:\n",
    "                self.dropout_slider.value = loaded_config['dropout']\n",
    "            if 'epochs' in loaded_config:\n",
    "                self.epochs_slider.value = loaded_config['epochs']\n",
    "            if 'learning_rate' in loaded_config:\n",
    "                self.learning_rate_slider.value = loaded_config['learning_rate']\n",
    "            if 'batch_size' in loaded_config:\n",
    "                self.batch_size_dropdown.value = loaded_config['batch_size']\n",
    "            if 'max_train_samples' in loaded_config:\n",
    "                self.max_train_samples_slider.value = loaded_config['max_train_samples']\n",
    "            if 'max_val_samples' in loaded_config:\n",
    "                self.max_val_samples_slider.value = loaded_config['max_val_samples']\n",
    "            if 'training_strategy' in loaded_config:\n",
    "                self.training_strategy_dropdown.value = loaded_config['training_strategy']\n",
    "            \n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(f\"✅ Configuration loaded from {config_file.name}\")\n",
    "                print(\"📋 Loaded parameters:\")\n",
    "                for key, value in loaded_config.items():\n",
    "                    if key != 'data_path':  # Skip data_path as it's auto-set\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(f\"❌ Error loading configuration: {str(e)}\")\n",
    "\n",
    "    def _save_config(self, button):\n",
    "        \"\"\"Save current training configuration to config directory\"\"\"\n",
    "        config = self._get_training_config()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save to config directory instead of model directory\n",
    "        config_file = self.config_dir / f\"dashboard_config_{timestamp}.yaml\"\n",
    "        \n",
    "        # Ensure config directory exists\n",
    "        self.config_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert to YAML format for better readability\n",
    "        try:\n",
    "            import yaml\n",
    "            with open(config_file, 'w') as f:\n",
    "                yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "            \n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(f\"💾 Configuration saved successfully!\")\n",
    "                print(f\"📂 Location: {config_file}\")\n",
    "                print(\"🔧 Configuration contents:\")\n",
    "                for key, value in config.items():\n",
    "                    print(f\"   {key}: {value}\")\n",
    "                    \n",
    "        except ImportError:\n",
    "            # Fallback to JSON if PyYAML not available\n",
    "            config_file = config_file.with_suffix('.json')\n",
    "            with open(config_file, 'w') as f:\n",
    "                import json\n",
    "                json.dump(config, f, indent=2)\n",
    "            \n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(f\"💾 Configuration saved to {config_file}\")\n",
    "                \n",
    "            # Refresh the config list to show the new file\n",
    "            self._refresh_config_list()\n",
    "        \n",
    "        except Exception as e:\n",
    "            with self.training_log_output:\n",
    "                clear_output()\n",
    "                print(f\"❌ Error saving configuration: {str(e)}\")\n",
    "    \n",
    "    def _get_training_config(self):\n",
    "        \"\"\"Get current training configuration from UI\"\"\"\n",
    "        return {\n",
    "            'data_path': self.data_dir,\n",
    "            'hidden_size': self.hidden_size_slider.value,\n",
    "            'num_layers': self.num_layers_slider.value,\n",
    "            'num_heads': self.num_heads_slider.value,\n",
    "            'dropout': self.dropout_slider.value,\n",
    "            'epochs': self.epochs_slider.value,\n",
    "            'learning_rate': self.learning_rate_slider.value,\n",
    "            'batch_size': self.batch_size_dropdown.value,\n",
    "            'max_train_samples': self.max_train_samples_slider.value,\n",
    "            'max_val_samples': self.max_val_samples_slider.value,\n",
    "            'weight_decay': 0.01,\n",
    "            'training_strategy': self.training_strategy_dropdown.value\n",
    "        }\n",
    "    \n",
    "    def _refresh_config_list(self, button=None):\n",
    "        \"\"\"Refresh the list of available training configurations\"\"\"\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Search for JSON config files in the model directory\n",
    "        if not self.model_dir.exists():\n",
    "            config_files = []\n",
    "        else:\n",
    "            config_files = list(self.model_dir.glob(\"*.json\"))\n",
    "        \n",
    "        # Extract config names (without .json extension)\n",
    "        config_names = [f.stem for f in config_files]\n",
    "        \n",
    "        if not config_names:\n",
    "            config_names = ['No configs found']\n",
    "            self.config_list = []\n",
    "        else:\n",
    "            self.config_list = config_names\n",
    "        \n",
    "        self.config_dropdown.options = config_names\n",
    "        \n",
    "        # Update status if we have a system info output\n",
    "        if hasattr(self, 'system_info_output'):\n",
    "            with self.system_info_output:\n",
    "                clear_output()\n",
    "                print(f\"📁 Found {len(config_files)} config files\")\n",
    "                if config_files:\n",
    "                    print(\"📋 Available configs:\")\n",
    "                    for config in config_names:\n",
    "                        print(f\"  • {config}\")\n",
    "    \n",
    "    def _refresh_model_list(self, button=None):\n",
    "        \"\"\"Refresh the list of available models from multiple directories\"\"\"\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Search in multiple locations\n",
    "        search_locations = [\n",
    "            self.model_dir,  # Original location (/models/)\n",
    "            Path(\"/Users/robertburkhall/Development/HRM/checkpoints\"),\n",
    "            Path(\"/Users/robertburkhall/Development/HRM/notebooks/colab\")\n",
    "        ]\n",
    "        \n",
    "        all_model_files = []\n",
    "        for location in search_locations:\n",
    "            if location.exists():\n",
    "                model_files = list(location.rglob(\"*.pt\"))\n",
    "                all_model_files.extend(model_files)\n",
    "        \n",
    "        # Create mapping of model names to full paths (avoiding duplicates)\n",
    "        unique_models = {}\n",
    "        for f in all_model_files:\n",
    "            model_name = f.stem\n",
    "            if model_name not in unique_models:\n",
    "                unique_models[model_name] = f\n",
    "        \n",
    "        model_names = list(unique_models.keys())\n",
    "        \n",
    "        if not model_names:\n",
    "            model_names = ['No models found']\n",
    "            self._model_paths = {}\n",
    "        else:\n",
    "            self._model_paths = unique_models\n",
    "        \n",
    "        self.model_dropdown.options = model_names\n",
    "        \n",
    "        with self.model_info_output:\n",
    "            clear_output()\n",
    "            print(f\"📁 Found {len(all_model_files)} total model files\")\n",
    "            print(f\"🎯 Unique models: {len(model_names) if model_names != ['No models found'] else 0}\")\n",
    "            for location in search_locations:\n",
    "                if location.exists():\n",
    "                    count = len(list(location.rglob(\"*.pt\")))\n",
    "                    if count > 0:\n",
    "                        print(f\"   📂 {location.name}: {count} models\")\n",
    "    \n",
    "    def _on_model_selected(self, change):\n",
    "        \"\"\"Handle model selection using stored full paths\"\"\"\n",
    "        if change['new'] == 'No models found':\n",
    "            return\n",
    "            \n",
    "        # Use stored full path instead of constructing one\n",
    "        if hasattr(self, '_model_paths') and change['new'] in self._model_paths:\n",
    "            model_path = self._model_paths[change['new']]\n",
    "        else:\n",
    "            # Fallback to original behavior\n",
    "            model_path = self.model_dir / f\"{change['new']}.pt\"\n",
    "        \n",
    "        with self.model_info_output:\n",
    "            clear_output()\n",
    "            self._display_model_info(model_path)\n",
    "    \n",
    "    def _display_model_info(self, model_path):\n",
    "        \"\"\"Display information about selected model\"\"\"\n",
    "        if not model_path.exists():\n",
    "            print(f\"❌ Model file not found: {model_path}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Load model metadata\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')\n",
    "            \n",
    "            print(f\"📊 Model Information: {model_path.name}\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            if 'model_config' in checkpoint:\n",
    "                config = checkpoint['model_config']\n",
    "                print(\"🏗️ Architecture:\")\n",
    "                for key, value in config.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            \n",
    "            if 'training_metrics' in checkpoint:\n",
    "                metrics = checkpoint['training_metrics']\n",
    "                print(\"\\\\n📈 Training Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        print(f\"  {key}: {value:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # File information\n",
    "            stat = model_path.stat()\n",
    "            print(f\"\\\\n📁 File Info:\")\n",
    "            print(f\"  Size: {stat.st_size / (1024*1024):.2f} MB\")\n",
    "            print(f\"  Modified: {datetime.fromtimestamp(stat.st_mtime)}\")\n",
    "            \n",
    "            # Calculate parameter count\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                total_params = sum(p.numel() for p in checkpoint['model_state_dict'].values())\n",
    "                print(f\"  Parameters: {total_params:,}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading model info: {str(e)}\")\n",
    "    \n",
    "    def _load_selected_model(self, button):\n",
    "        \"\"\"Load the selected model\"\"\"\n",
    "        model_name = self.model_dropdown.value\n",
    "        if model_name == 'No models found':\n",
    "            return\n",
    "        \n",
    "        with self.testing_output:\n",
    "            print(f\"📥 Loading model: {model_name}\")\n",
    "            # TODO: Implement model loading logic\n",
    "            print(\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    def _test_selected_model(self, button):\n",
    "        \"\"\"Test the selected model on sample data\"\"\"\n",
    "        model_name = self.model_dropdown.value\n",
    "        if model_name == 'No models found':\n",
    "            return\n",
    "        \n",
    "        # Use stored full path instead of constructing wrong path\n",
    "        if hasattr(self, '_model_paths') and model_name in self._model_paths:\n",
    "            model_path = self._model_paths[model_name]\n",
    "        else:\n",
    "            # Fallback to original behavior\n",
    "            model_path = self.model_dir / f\"{model_name}.pt\"\n",
    "        \n",
    "        with self.testing_output:\n",
    "            clear_output()\n",
    "            print(f\"🧪 Testing model: {model_name}\")\n",
    "            print(f\"📍 Model path: {model_path}\")\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                print(f\"❌ Model file not found: {model_path}\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load the model\n",
    "                print(\"🔄 Loading model...\")\n",
    "                checkpoint = torch.load(model_path, map_location=self.device)\n",
    "                \n",
    "                # Display model info\n",
    "                print(f\"✅ Model loaded successfully!\")\n",
    "                print(f\"📊 Model size: {model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "                \n",
    "                if isinstance(checkpoint, dict):\n",
    "                    # Check what's in the checkpoint\n",
    "                    print(\"🔍 Checkpoint contents:\")\n",
    "                    for key in checkpoint.keys():\n",
    "                        if key == 'model_state_dict' and isinstance(checkpoint[key], dict):\n",
    "                            print(f\"   📦 {key}: {len(checkpoint[key])} parameters\")\n",
    "                        elif key == 'epoch':\n",
    "                            print(f\"   📅 {key}: {checkpoint[key]}\")\n",
    "                        elif key in ['accuracy', 'loss', 'val_accuracy', 'val_loss']:\n",
    "                            print(f\"   📈 {key}: {checkpoint[key]:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"   📋 {key}: {type(checkpoint[key])}\")\n",
    "                \n",
    "                # Basic validation test\n",
    "                if 'val_accuracy' in checkpoint:\n",
    "                    val_acc = checkpoint['val_accuracy']\n",
    "                    print(f\"\\n🎯 Model Performance:\")\n",
    "                    print(f\"   Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "                    \n",
    "                    if val_acc > 0.8:\n",
    "                        print(\"   🌟 Excellent performance!\")\n",
    "                    elif val_acc > 0.6:\n",
    "                        print(\"   ✅ Good performance\")\n",
    "                    elif val_acc > 0.4:\n",
    "                        print(\"   ⚠️  Moderate performance\")\n",
    "                    else:\n",
    "                        print(\"   📈 Needs improvement\")\n",
    "                \n",
    "                print(\"\\n✅ Model testing completed!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error testing model: {str(e)}\")\n",
    "                import traceback\n",
    "                print(\"🔍 Error details:\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def _delete_selected_model(self, button):\n",
    "        \"\"\"Delete the selected model using correct path mapping\"\"\"\n",
    "        model_name = self.model_dropdown.value\n",
    "        if model_name == 'No models found':\n",
    "            return\n",
    "        \n",
    "        # Use stored full path instead of constructing wrong path\n",
    "        if hasattr(self, '_model_paths') and model_name in self._model_paths:\n",
    "            model_path = self._model_paths[model_name]\n",
    "        else:\n",
    "            # Fallback to original behavior\n",
    "            model_path = self.model_dir / f\"{model_name}.pt\"\n",
    "        \n",
    "        # Confirmation would be nice here\n",
    "        try:\n",
    "            model_path.unlink()\n",
    "            with self.testing_output:\n",
    "                print(f\"🗑️ Deleted model: {model_name}\")\n",
    "                print(f\"📍 Removed file: {model_path}\")\n",
    "            self._refresh_model_list()\n",
    "        except Exception as e:\n",
    "            with self.testing_output:\n",
    "                print(f\"❌ Error deleting model: {str(e)}\")\n",
    "                print(f\"📍 Attempted path: {model_path}\")\n",
    "    \n",
    "    def _compare_models(self, button):\n",
    "        \"\"\"Compare multiple models\"\"\"\n",
    "        with self.comparison_output:\n",
    "            clear_output()\n",
    "            print(\"📊 Model Comparison\")\n",
    "            print(\"=\" * 30)\n",
    "            \n",
    "            model_files = list(self.model_dir.glob(\"*.pt\"))\n",
    "            if len(model_files) < 2:\n",
    "                print(\"❌ Need at least 2 models for comparison\")\n",
    "                return\n",
    "            \n",
    "            # TODO: Implement model comparison logic\n",
    "            print(f\"Comparing {len(model_files)} models...\")\n",
    "            for model_file in model_files:\n",
    "                print(f\"  📁 {model_file.stem}\")\n",
    "    \n",
    "    def _generate_visualization(self, button):\n",
    "        \"\"\"Generate the selected visualization\"\"\"\n",
    "        viz_type = self.viz_type_dropdown.value\n",
    "        display_mode = self.display_mode_toggle.value\n",
    "        \n",
    "        with self.viz_output:\n",
    "            clear_output()\n",
    "            print(f\"📊 Generating {viz_type} in {display_mode} mode\")\n",
    "            \n",
    "            if viz_type == 'Training Metrics Graph':\n",
    "                self._plot_training_metrics(display_mode)\n",
    "            elif viz_type == 'Model Architecture Diagram':\n",
    "                self._plot_model_architecture(display_mode)\n",
    "            elif viz_type == 'Dataset Distribution':\n",
    "                self._plot_dataset_distribution(display_mode)\n",
    "            elif viz_type == 'Performance Heatmap':\n",
    "                self._plot_performance_heatmap(display_mode)\n",
    "            elif viz_type == 'Error Analysis':\n",
    "                self._plot_error_analysis(display_mode)\n",
    "            elif viz_type == 'Learning Curves':\n",
    "                self._plot_learning_curves(display_mode)\n",
    "    \n",
    "    def _plot_training_metrics(self, display_mode):\n",
    "        \"\"\"Plot training metrics\"\"\"\n",
    "        if display_mode in ['Graphical', 'Both']:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Dummy data for demonstration\n",
    "            epochs = list(range(1, 11))\n",
    "            train_loss = [2.5 - i*0.2 + np.random.normal(0, 0.1) for i in epochs]\n",
    "            val_acc = [0.1 + i*0.08 + np.random.normal(0, 0.02) for i in epochs]\n",
    "            \n",
    "            ax1.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            ax2.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.set_title('Validation Accuracy')\n",
    "            ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        if display_mode in ['Text Summary', 'Both']:\n",
    "            print(\"\\\\n📈 Training Metrics Summary:\")\n",
    "            print(f\"  Final Loss: 0.45\")\n",
    "            print(f\"  Best Accuracy: 0.87\")\n",
    "            print(f\"  Training Time: 15.3 minutes\")\n",
    "    \n",
    "    def _plot_model_architecture(self, display_mode):\n",
    "        \"\"\"Plot model architecture diagram\"\"\"\n",
    "        if display_mode in ['Graphical', 'Both']:\n",
    "            import matplotlib.pyplot as plt\n",
    "            import matplotlib.patches as patches\n",
    "            from matplotlib.patches import FancyBboxPatch, Rectangle\n",
    "            \n",
    "            # Create figure and axis\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "            ax.set_xlim(0, 10)\n",
    "            ax.set_ylim(0, 10)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Title\n",
    "            ax.text(5, 9.5, 'HRM Sudoku Transformer Architecture', \n",
    "                   fontsize=16, ha='center', fontweight='bold')\n",
    "            \n",
    "            # Get current configuration values\n",
    "            hidden_size = self.hidden_size_slider.value\n",
    "            num_layers = self.num_layers_slider.value\n",
    "            num_heads = self.num_heads_slider.value\n",
    "            dropout = self.dropout_slider.value\n",
    "            \n",
    "            # Input Layer\n",
    "            input_box = FancyBboxPatch((1, 8.5), 8, 0.4, boxstyle=\"round,pad=0.05\", \n",
    "                                     facecolor='lightblue', edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(input_box)\n",
    "            ax.text(5, 8.7, f'Input: Sudoku Grid (9x9) → Tokenized', ha='center', fontweight='bold')\n",
    "            \n",
    "            # Embedding Layer\n",
    "            embed_box = FancyBboxPatch((2, 7.8), 6, 0.4, boxstyle=\"round,pad=0.05\",\n",
    "                                     facecolor='lightgreen', edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(embed_box)\n",
    "            ax.text(5, 8.0, f'Token Embedding + Position Encoding → {hidden_size}d', ha='center')\n",
    "            \n",
    "            # Transformer Layers\n",
    "            layer_height = 5.5 / max(num_layers, 1)  # Distribute layers in available space\n",
    "            for i in range(num_layers):\n",
    "                y_pos = 7.2 - i * layer_height\n",
    "                \n",
    "                # Main transformer block\n",
    "                layer_box = FancyBboxPatch((1.5, y_pos - layer_height/2 + 0.1), 7, layer_height - 0.2, \n",
    "                                         boxstyle=\"round,pad=0.05\", facecolor='lightyellow', \n",
    "                                         edgecolor='black', linewidth=1)\n",
    "                ax.add_patch(layer_box)\n",
    "                \n",
    "                # Multi-head attention\n",
    "                attn_box = Rectangle((2, y_pos - layer_height/3), 6, layer_height/4, \n",
    "                                   facecolor='lightcoral', edgecolor='black', alpha=0.7)\n",
    "                ax.add_patch(attn_box)\n",
    "                ax.text(5, y_pos - layer_height/6, f'Multi-Head Attention ({num_heads} heads)', \n",
    "                       ha='center', fontsize=9, fontweight='bold')\n",
    "                \n",
    "                # Feed forward\n",
    "                ff_box = Rectangle((2, y_pos - 2*layer_height/3), 6, layer_height/4, \n",
    "                                 facecolor='lightsteelblue', edgecolor='black', alpha=0.7)\n",
    "                ax.add_patch(ff_box)\n",
    "                ax.text(5, y_pos - layer_height/2 - layer_height/12, f'Feed Forward + Dropout ({dropout})', \n",
    "                       ha='center', fontsize=9)\n",
    "                \n",
    "                # Layer number\n",
    "                ax.text(0.8, y_pos - layer_height/4, f'L{i+1}', ha='center', fontweight='bold', \n",
    "                       fontsize=10, bbox=dict(boxstyle=\"circle\", facecolor='white'))\n",
    "            \n",
    "            # Output Layer\n",
    "            output_y = 7.2 - num_layers * layer_height - 0.3\n",
    "            output_box = FancyBboxPatch((2, output_y), 6, 0.4, boxstyle=\"round,pad=0.05\",\n",
    "                                      facecolor='orange', edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(output_box)\n",
    "            ax.text(5, output_y + 0.2, f'Linear Layer → 10 classes (0-9)', ha='center', fontweight='bold')\n",
    "            \n",
    "            # Final prediction\n",
    "            final_y = output_y - 0.8\n",
    "            final_box = FancyBboxPatch((1, final_y), 8, 0.4, boxstyle=\"round,pad=0.05\",\n",
    "                                     facecolor='lightpink', edgecolor='black', linewidth=1)\n",
    "            ax.add_patch(final_box)\n",
    "            ax.text(5, final_y + 0.2, f'Output: Filled Sudoku Grid (9x9)', ha='center', fontweight='bold')\n",
    "            \n",
    "            # Add arrows\n",
    "            arrow_props = dict(arrowstyle='->', lw=2, color='darkblue')\n",
    "            # Input to embedding\n",
    "            ax.annotate('', xy=(5, 7.8), xytext=(5, 8.5), arrowprops=arrow_props)\n",
    "            # Between transformer layers\n",
    "            for i in range(num_layers):\n",
    "                y_start = 7.2 - i * layer_height - layer_height/2 + 0.1\n",
    "                if i == 0:\n",
    "                    ax.annotate('', xy=(5, y_start + layer_height/2 - 0.1), xytext=(5, 7.8), arrowprops=arrow_props)\n",
    "                if i < num_layers - 1:\n",
    "                    y_end = 7.2 - (i + 1) * layer_height + layer_height/2 - 0.1\n",
    "                    ax.annotate('', xy=(5, y_end), xytext=(5, y_start), arrowprops=arrow_props)\n",
    "            \n",
    "            # Last layer to output\n",
    "            last_layer_y = 7.2 - (num_layers - 1) * layer_height - layer_height/2 + 0.1\n",
    "            ax.annotate('', xy=(5, output_y + 0.4), xytext=(5, last_layer_y), arrowprops=arrow_props)\n",
    "            ax.annotate('', xy=(5, final_y + 0.4), xytext=(5, output_y), arrowprops=arrow_props)\n",
    "            \n",
    "            # Add configuration summary box\n",
    "            config_text = f\"Configuration:\\n• Hidden Size: {hidden_size}\\n• Layers: {num_layers}\\n• Attention Heads: {num_heads}\\n• Dropout: {dropout}\"\n",
    "            ax.text(9.5, 8, config_text, fontsize=9, ha='left', va='top',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        if display_mode in ['Text Summary', 'Both']:\n",
    "            print(\"\\n🏗️ Model Architecture Summary:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(f\"🔧 Model Type: HRM Sudoku Transformer\")\n",
    "            print(f\"📊 Architecture Details:\")\n",
    "            print(f\"   • Hidden Dimension: {self.hidden_size_slider.value}\")\n",
    "            print(f\"   • Transformer Layers: {self.num_layers_slider.value}\")\n",
    "            print(f\"   • Attention Heads: {self.num_heads_slider.value}\")\n",
    "            print(f\"   • Dropout Rate: {self.dropout_slider.value}\")\n",
    "            print(f\"   • Input Shape: (batch_size, 81)  # 9x9 flattened\")\n",
    "            print(f\"   • Output Shape: (batch_size, 81, 10)  # 10 possible values per cell\")\n",
    "            \n",
    "            # Calculate approximate parameter count\n",
    "            embed_params = 81 * self.hidden_size_slider.value  # Position + token embedding\n",
    "            layer_params = self.num_layers_slider.value * (\n",
    "                # Multi-head attention\n",
    "                4 * self.hidden_size_slider.value * self.hidden_size_slider.value +\n",
    "                # Feed forward\n",
    "                2 * self.hidden_size_slider.value * self.hidden_size_slider.value * 4\n",
    "            )\n",
    "            output_params = self.hidden_size_slider.value * 10 * 81  # Output projection\n",
    "            total_params = embed_params + layer_params + output_params\n",
    "            \n",
    "            print(f\"📈 Estimated Parameters: ~{total_params:,}\")\n",
    "            print(f\"💾 Model Size: ~{total_params * 4 / (1024*1024):.1f} MB (fp32)\")\n",
    "            print(\"=\" * 40)\n",
    "    \n",
    "    def _plot_dataset_distribution(self, display_mode):\n",
    "        \"\"\"Plot dataset distribution\"\"\"\n",
    "        print(\"📊 Dataset Distribution Analysis:\")\n",
    "        print(\"  Training samples: 50,000\")\n",
    "        print(\"  Validation samples: 10,000\")\n",
    "        print(\"  Test samples: 5,000\")\n",
    "    \n",
    "    def _plot_performance_heatmap(self, display_mode):\n",
    "        \"\"\"Plot performance heatmap\"\"\"\n",
    "        print(\"🌡️ Performance Heatmap (by difficulty):\")\n",
    "        print(\"  Easy: 95% accuracy\")\n",
    "        print(\"  Medium: 78% accuracy\") \n",
    "        print(\"  Hard: 45% accuracy\")\n",
    "    \n",
    "    def _plot_error_analysis(self, display_mode):\n",
    "        \"\"\"Plot error analysis\"\"\"\n",
    "        print(\"🔍 Error Analysis:\")\n",
    "        print(\"  Most common errors: Box constraint violations\")\n",
    "        print(\"  Error rate by position: Corner cells have higher accuracy\")\n",
    "    \n",
    "    def _plot_learning_curves(self, display_mode):\n",
    "        \"\"\"Plot learning curves\"\"\"\n",
    "        if display_mode in ['Graphical', 'Both']:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "            \n",
    "            # Dummy learning curves\n",
    "            train_sizes = [100, 500, 1000, 2000, 5000]\n",
    "            train_scores = [0.6, 0.75, 0.82, 0.85, 0.87]\n",
    "            val_scores = [0.58, 0.72, 0.78, 0.81, 0.83]\n",
    "            \n",
    "            ax.plot(train_sizes, train_scores, 'o-', label='Training Score')\n",
    "            ax.plot(train_sizes, val_scores, 's-', label='Validation Score')\n",
    "            ax.set_xlabel('Training Set Size')\n",
    "            ax.set_ylabel('Accuracy Score')\n",
    "            ax.set_title('Learning Curves')\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def _clear_output(self, button):\n",
    "        \"\"\"Clear all output sections\"\"\"\n",
    "        self.training_log_output.clear_output()\n",
    "        self.testing_output.clear_output()\n",
    "        self.metrics_output.clear_output()\n",
    "        print(\"🗑️ All outputs cleared\")\n",
    "    \n",
    "    def _export_logs(self, button):\n",
    "        \"\"\"Export logs to file\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = self.model_dir / f\"training_logs_{timestamp}.txt\"\n",
    "        \n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(f\"HRM Training Logs - {timestamp}\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\")\n",
    "            # TODO: Export actual log content\n",
    "        \n",
    "        print(f\"💾 Logs exported to {log_file}\")\n",
    "    \n",
    "    def _display_system_info(self):\n",
    "        \"\"\"Display system information\"\"\"\n",
    "        with self.system_info_output:\n",
    "            print(\"🖥️ System Information\")\n",
    "            print(\"=\" * 30)\n",
    "            print(f\"Device: {self.device}\")\n",
    "            print(f\"PyTorch Version: {torch.__version__}\")\n",
    "            print(f\"Python Version: {sys.version}\")\n",
    "            print(f\"Data Directory: {self.data_dir}\")\n",
    "            print(f\"Model Directory: {self.model_dir}\")\n",
    "            \n",
    "            if torch.backends.mps.is_available():\n",
    "                print(\"✅ MPS (Apple Silicon) acceleration available\")\n",
    "            elif torch.cuda.is_available():\n",
    "                print(\"✅ CUDA acceleration available\")\n",
    "            else:\n",
    "                print(\"⚠️ Using CPU only\")\n",
    "\n",
    "# Create the comprehensive dashboard\n",
    "comprehensive_dashboard = ComprehensiveHRMDashboard(\n",
    "    model=model if 'model' in locals() else None,\n",
    "    device=device,\n",
    "    data_dir=DATA_DIR,\n",
    "    model_dir=MODEL_DIR\n",
    ")\n",
    "\n",
    "print(\"🎯 Comprehensive HRM Dashboard created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba378d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the comprehensive dashboard\n",
    "comprehensive_dashboard = ComprehensiveHRMDashboard(\n",
    "    model=model if 'model' in locals() else None,\n",
    "    device=device,\n",
    "    data_dir=DATA_DIR,\n",
    "    model_dir=MODEL_DIR\n",
    ")\n",
    "\n",
    "print(\"🎯 Comprehensive HRM Dashboard created successfully!\")\n",
    "\n",
    "# Display the dashboard\n",
    "display(comprehensive_dashboard.dashboard)\n",
    "\n",
    "# Fix the _refresh_config_list method to scan the correct directory\n",
    "def fixed_refresh_config_list(self, button=None):\n",
    "    \"\"\"Refresh the list of available configuration files - FIXED VERSION\"\"\"\n",
    "    try:\n",
    "        config_files = []\n",
    "        if self.config_dir.exists():  # Changed from self.model_dir to self.config_dir\n",
    "            # Look for YAML config files (changed from JSON to YAML)\n",
    "            for config_file in self.config_dir.glob(\"*.yaml\"):\n",
    "                if \"config\" in config_file.name.lower():\n",
    "                    config_files.append(config_file.name)\n",
    "\n",
    "        if config_files:\n",
    "            # Sort by modification time (newest first)\n",
    "            config_files.sort(key=lambda x: (self.config_dir / x).stat().st_mtime, reverse=True)\n",
    "            self.config_dropdown.options = config_files\n",
    "        else:\n",
    "            self.config_dropdown.options = ['No configs found']\n",
    "\n",
    "    except Exception as e:\n",
    "        self.config_dropdown.options = [f'Error: {str(e)}']\n",
    "\n",
    "    # Use system_info_output for feedback\n",
    "    with self.system_info_output:\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()\n",
    "        print(f\"📁 Found {len(config_files) if 'config_files' in locals() else 0} config files\")\n",
    "        print(f\"📂 Config directory: {self.config_dir}\")\n",
    "\n",
    "# Replace the method\n",
    "print(\"🔧 Fixing _refresh_config_list method...\")\n",
    "comprehensive_dashboard._refresh_config_list = fixed_refresh_config_list.__get__(comprehensive_dashboard, comprehensive_dashboard.__class__)\n",
    "\n",
    "print(\"✅ Method fixed successfully!\")\n",
    "\n",
    "# Test the fixed method\n",
    "print(\"\\n🧪 Testing the fixed config refresh...\")\n",
    "try:\n",
    "    comprehensive_dashboard._refresh_config_list()\n",
    "    print(\"✅ Fixed method executed successfully!\")\n",
    "    print(f\"📋 Config dropdown options: {comprehensive_dashboard.config_dropdown.options}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fixed method failed: {e}\")\n",
    "\n",
    "print(\"🎉 Config dropdown should now show your saved configs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f3ae4",
   "metadata": {},
   "source": [
    "# 🎓 Understanding HRM Training - A Beginner's Guide\n",
    "\n",
    "## What Makes HRM Different from Traditional Neural Networks?\n",
    "\n",
    "**HRM (Hierarchical Reasoning Model)** is fundamentally different from typical neural networks like image classifiers or language models. Here's why:\n",
    "\n",
    "### 1. **Problem-Solving vs Pattern Recognition**\n",
    "- **Traditional Neural Networks**: Learn to recognize patterns (like \"this image contains a cat\")\n",
    "- **HRM**: Learns to **reason through multi-step problems** (like solving a Sudoku puzzle step by step)\n",
    "\n",
    "### 2. **Sequential Reasoning vs One-Shot Prediction**\n",
    "- **Traditional**: Input → Process → Single Output\n",
    "- **HRM**: Input → **Think Step 1** → **Think Step 2** → **Think Step 3** → ... → Final Solution\n",
    "\n",
    "### 3. **Working Memory**\n",
    "- **Traditional**: No memory of intermediate steps\n",
    "- **HRM**: Maintains a \"working memory\" of partial solutions and reasoning states\n",
    "\n",
    "## What Happens When You Click \"Start Training\"?\n",
    "\n",
    "### Phase 1: **Data Loading** 📚\n",
    "```\n",
    "🔍 Loading Sudoku puzzles from your dataset...\n",
    "📊 Training Set: ~45,000 puzzles\n",
    "📊 Validation Set: ~7,500 puzzles\n",
    "```\n",
    "- Each puzzle is a 9×9 grid with some numbers filled in (clues)\n",
    "- The model needs to learn to fill in the missing numbers\n",
    "\n",
    "### Phase 2: **Model Architecture Setup** 🏗️\n",
    "```\n",
    "🧠 Hidden Size: 256 dimensions (adjustable in your dashboard)\n",
    "🔗 Transformer Layers: 4 layers (how deep the reasoning goes)\n",
    "👁️ Attention Heads: 8 heads (parallel reasoning streams)\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "- **Hidden Size**: How much \"brain capacity\" each reasoning step has\n",
    "- **Layers**: How many reasoning steps the model can take\n",
    "- **Attention**: How well it can focus on different parts of the puzzle\n",
    "\n",
    "### Phase 3: **The Training Loop** 🔄\n",
    "\n",
    "For each puzzle, here's what happens:\n",
    "\n",
    "#### Step 1: **Puzzle Input**\n",
    "```\n",
    "Input: | . . 1 | . . 7 | . 2 . |\n",
    "       | 8 . . | . . . | . . 4 |\n",
    "       | 6 9 . | . . . | . . . |\n",
    "       | ...                   |\n",
    "```\n",
    "\n",
    "#### Step 2: **Hierarchical Reasoning**\n",
    "The model thinks in layers:\n",
    "- **Layer 1**: \"What numbers can go in each empty cell?\"\n",
    "- **Layer 2**: \"Which placements would be consistent?\"\n",
    "- **Layer 3**: \"What's the logical next step?\"\n",
    "- **Layer 4**: \"How does this affect other cells?\"\n",
    "\n",
    "#### Step 3: **Prediction & Learning**\n",
    "```\n",
    "Model's attempt: | 5 . 1 | . . 7 | . 2 . |\n",
    "Correct answer:  | 5 4 1 | 3 8 7 | 9 2 6 |\n",
    "                    ↑\n",
    "                  Error here - learn from this!\n",
    "```\n",
    "\n",
    "#### Step 4: **Backpropagation** (Learning)\n",
    "- Model realizes: \"I put the wrong number in position (1,2)\"\n",
    "- Adjusts internal reasoning: \"Next time, consider row constraints better\"\n",
    "\n",
    "### Phase 4: **Progress Monitoring** 📈\n",
    "\n",
    "You'll see metrics like:\n",
    "- **Loss**: How \"wrong\" the model's guesses are (lower = better)\n",
    "- **Accuracy**: Percentage of cells filled correctly\n",
    "- **Valid Solutions**: Percentage of complete, rule-following puzzles\n",
    "\n",
    "## Why is HRM Training Slow?\n",
    "\n",
    "HRM training is more complex than typical neural networks because:\n",
    "\n",
    "1. **Sequential Dependencies**: Each reasoning step depends on previous steps\n",
    "2. **Long-Range Planning**: Must consider consequences many steps ahead\n",
    "3. **Constraint Satisfaction**: Must learn Sudoku rules (no repeats in rows/columns/boxes)\n",
    "4. **Search Strategy**: Must learn when to guess vs when to deduce logically\n",
    "\n",
    "## What You'll Observe During Training\n",
    "\n",
    "### Early Epochs (1-5):\n",
    "```\n",
    "💭 Model thinking: \"I'll just put random numbers everywhere\"\n",
    "📊 Loss: ~2.8 (very high)\n",
    "🎯 Accuracy: ~10% (terrible)\n",
    "```\n",
    "\n",
    "### Middle Epochs (10-20):\n",
    "```\n",
    "💭 Model thinking: \"I'm learning the rules - no repeating numbers!\"\n",
    "📊 Loss: ~1.5 (improving)\n",
    "🎯 Accuracy: ~40% (getting better)\n",
    "```\n",
    "\n",
    "### Later Epochs (30+):\n",
    "```\n",
    "💭 Model thinking: \"I can solve easy puzzles and make good guesses on hard ones\"\n",
    "📊 Loss: ~0.8 (much better)\n",
    "🎯 Accuracy: ~70% (pretty good!)\n",
    "```\n",
    "\n",
    "## Your Dashboard Settings Explained\n",
    "\n",
    "### **Epochs**: \n",
    "- How many times the model sees the entire dataset\n",
    "- More epochs = more learning opportunities\n",
    "- Too many = overfitting (memorizing instead of reasoning)\n",
    "\n",
    "### **Learning Rate**:\n",
    "- How big steps the model takes when learning\n",
    "- Too high: Model learns too fast, might miss optimal solutions\n",
    "- Too low: Model learns too slowly, might get stuck\n",
    "\n",
    "### **Batch Size**:\n",
    "- How many puzzles the model works on simultaneously\n",
    "- Larger batches: More stable learning, needs more memory\n",
    "- Smaller batches: More frequent updates, less memory\n",
    "\n",
    "## Ready to Start?\n",
    "\n",
    "When you're ready to click \"Start Training\", the model will begin this fascinating journey from random number placement to logical puzzle solving. The training will show you real-time progress, and you can watch as your HRM develops reasoning capabilities!\n",
    "\n",
    "Take your time to absorb this - it's quite different from traditional machine learning, and that's what makes it so interesting! 🧠✨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd5e375",
   "metadata": {},
   "source": [
    "# 🚀 Your First Training Run - What to Expect\n",
    "\n",
    "## Before You Click \"Start Training\"\n",
    "\n",
    "### Current Dashboard Settings Review:\n",
    "- **Model Name**: Will be automatically generated with timestamp\n",
    "- **Epochs**: Default is probably 10-20 (good for first run)\n",
    "- **Learning Rate**: Default ~0.001 (conservative, safe choice)\n",
    "- **Batch Size**: Depends on your Mac's memory (probably 8-16)\n",
    "\n",
    "## Timeline for First Training Run\n",
    "\n",
    "### **Minutes 1-2: Initialization** ⚡\n",
    "```\n",
    "✅ Loading dataset... (45,000 training puzzles)\n",
    "✅ Creating model architecture... (256 hidden dimensions)\n",
    "✅ Setting up optimizer and loss function...\n",
    "✅ Preparing training batches...\n",
    "```\n",
    "\n",
    "### **Minutes 2-10: Early Learning** 🐣\n",
    "```\n",
    "Epoch 1/20: Loss: 2.89 | Accuracy: 12.3% | Time: ~30s\n",
    "Epoch 2/20: Loss: 2.45 | Accuracy: 18.7% | Time: ~30s\n",
    "Epoch 3/20: Loss: 2.21 | Accuracy: 24.1% | Time: ~30s\n",
    "```\n",
    "**What's happening**: Model is learning basic Sudoku rules\n",
    "\n",
    "### **Minutes 10-15: Pattern Recognition** 🧩\n",
    "```\n",
    "Epoch 5/20: Loss: 1.87 | Accuracy: 32.4% | Time: ~30s\n",
    "Epoch 7/20: Loss: 1.52 | Accuracy: 41.8% | Time: ~30s\n",
    "```\n",
    "**What's happening**: Model recognizes number constraints in rows/columns\n",
    "\n",
    "### **Minutes 15-20: Strategic Reasoning** 🎯\n",
    "```\n",
    "Epoch 12/20: Loss: 1.18 | Accuracy: 52.7% | Time: ~30s\n",
    "Epoch 15/20: Loss: 0.94 | Accuracy: 61.3% | Time: ~30s\n",
    "```\n",
    "**What's happening**: Model develops multi-step reasoning strategies\n",
    "\n",
    "### **Minutes 20-25: Fine-tuning** ✨\n",
    "```\n",
    "Epoch 18/20: Loss: 0.78 | Accuracy: 67.9% | Time: ~30s\n",
    "Epoch 20/20: Loss: 0.71 | Accuracy: 71.2% | Time: ~30s\n",
    "```\n",
    "**What's happening**: Model polishes its reasoning approach\n",
    "\n",
    "## Success Indicators for First Run\n",
    "\n",
    "### 🟢 **Excellent Results** (You're doing great!):\n",
    "- Final accuracy: 65%+\n",
    "- Loss drops consistently\n",
    "- No error messages\n",
    "\n",
    "### 🟡 **Good Results** (Solid progress):\n",
    "- Final accuracy: 45-65%\n",
    "- Loss mostly decreasing\n",
    "- Minor warnings only\n",
    "\n",
    "### 🔴 **Issues to Watch For**:\n",
    "- Accuracy stuck below 30%\n",
    "- Loss not decreasing after epoch 5\n",
    "- Memory errors or crashes\n",
    "\n",
    "## What the Graphs Will Show\n",
    "\n",
    "### **Loss Graph**: 📉\n",
    "- Should start high (~3.0) and decrease\n",
    "- Might have some bumps (normal!)\n",
    "- Smooth downward trend is ideal\n",
    "\n",
    "### **Accuracy Graph**: 📈\n",
    "- Should start low (~10-15%) and increase\n",
    "- Steeper climb in early epochs\n",
    "- Levels off around 60-70% (excellent for first run)\n",
    "\n",
    "## After Training Completes\n",
    "\n",
    "### Automatic Actions:\n",
    "```\n",
    "✅ Model automatically saved to checkpoints/\n",
    "✅ Training graphs displayed\n",
    "✅ Final metrics calculated\n",
    "✅ Best model weights preserved\n",
    "```\n",
    "\n",
    "### What You Can Do Next:\n",
    "1. **Evaluate**: Test on validation puzzles\n",
    "2. **Visualize**: See how model solves specific puzzles\n",
    "3. **Adjust**: Tweak settings for next run\n",
    "4. **Compare**: Run different configurations\n",
    "\n",
    "## Pro Tips for First-Time Success\n",
    "\n",
    "### 1. **Let It Run Uninterrupted**\n",
    "- Don't click other buttons during training\n",
    "- Keep your Mac awake (training takes ~25 minutes)\n",
    "- Watch the progress bars - they're quite satisfying!\n",
    "\n",
    "### 2. **Expected Performance Benchmarks**\n",
    "- **Beginner Model** (your first run): 60-70% accuracy\n",
    "- **Good Model** (after tuning): 75-85% accuracy  \n",
    "- **Expert Model** (lots of training): 90%+ accuracy\n",
    "\n",
    "### 3. **Don't Worry If...**\n",
    "- Loss jumps around a bit (normal!)\n",
    "- Progress seems slow at first (reasoning is complex!)\n",
    "- Some epochs take longer (dynamic batch processing)\n",
    "\n",
    "## Memory Usage (Your Mac)\n",
    "\n",
    "With your Apple Silicon Mac and MPS acceleration:\n",
    "- **Expected RAM usage**: 4-8GB during training\n",
    "- **Expected training time**: 20-30 minutes for 20 epochs\n",
    "- **GPU utilization**: Model will use your Mac's Neural Engine\n",
    "\n",
    "## Ready to Begin Your HRM Journey?\n",
    "\n",
    "Your model is about to learn one of the most complex forms of reasoning - logical constraint satisfaction! This is much more sophisticated than typical \"cat vs dog\" classification. \n",
    "\n",
    "When you click \"Start Training\", you're starting a journey from random number placement to systematic logical reasoning. Enjoy watching your AI develop problem-solving capabilities! 🧠🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ee62d",
   "metadata": {},
   "source": [
    "# 🏗️ HRM → Software Engineering Model: Transformation Plan\n",
    "\n",
    "## Vision: \"Assignments System of Work\" AI Engineer\n",
    "\n",
    "### **Current State vs Target State**\n",
    "\n",
    "#### **Current HRM Capabilities** 🧩\n",
    "- **Domain**: Constraint satisfaction (9×9 Sudoku grids)\n",
    "- **Reasoning**: Rule-based logical deduction\n",
    "- **Input/Output**: Numbers in fixed positions\n",
    "- **Complexity**: Single problem type, clear rules\n",
    "\n",
    "#### **Target SE Model Capabilities** 🏗️\n",
    "- **Domain**: Software engineering across multiple languages/frameworks\n",
    "- **Reasoning**: Multi-layered architectural, procedural, and creative thinking\n",
    "- **Input/Output**: Requirements → Code + Documentation + Tests + Architecture\n",
    "- **Complexity**: Infinite problem variations, evolving best practices\n",
    "\n",
    "## 🎯 Software Engineering Training Requirements\n",
    "\n",
    "### **Core Competencies Needed**\n",
    "\n",
    "#### 1. **SDLC Mastery** 📋\n",
    "```\n",
    "Training Data Requirements:\n",
    "✅ Agile/Scrum methodologies and ceremonies\n",
    "✅ DevOps practices and CI/CD pipelines  \n",
    "✅ Requirements analysis and user story writing\n",
    "✅ Software architecture patterns and principles\n",
    "✅ Code review processes and quality gates\n",
    "✅ Testing strategies (unit, integration, E2E)\n",
    "✅ Documentation standards and practices\n",
    "✅ Version control workflows (Git best practices)\n",
    "```\n",
    "\n",
    "#### 2. **Multi-Language Code Generation** 💻\n",
    "```\n",
    "Primary Languages for Training:\n",
    "✅ Python (backend, data science, automation)\n",
    "✅ JavaScript/TypeScript (frontend, Node.js)\n",
    "✅ Java (enterprise applications)\n",
    "✅ C/C++ (systems programming, hardware interface)\n",
    "✅ SQL (database design and queries)\n",
    "✅ Shell scripting (deployment, automation)\n",
    "✅ Configuration languages (YAML, JSON, Terraform)\n",
    "```\n",
    "\n",
    "#### 3. **Architecture & Design Patterns** 🏛️\n",
    "```\n",
    "Design Competencies:\n",
    "✅ Microservices architecture\n",
    "✅ Database design and optimization\n",
    "✅ API design (REST, GraphQL, gRPC)\n",
    "✅ Security patterns and practices\n",
    "✅ Scalability and performance optimization\n",
    "✅ Cloud architecture (AWS, Azure, GCP)\n",
    "✅ Infrastructure as Code\n",
    "```\n",
    "\n",
    "### **Training Data Sources & Strategy**\n",
    "\n",
    "#### **High-Quality Code Repositories** 📚\n",
    "```python\n",
    "training_sources = {\n",
    "    \"open_source_projects\": [\n",
    "        \"GitHub top-starred repositories\",\n",
    "        \"Apache Foundation projects\", \n",
    "        \"CNCF projects\",\n",
    "        \"Popular framework repositories\"\n",
    "    ],\n",
    "    \"documentation\": [\n",
    "        \"Official language documentation\",\n",
    "        \"Framework guides and tutorials\",\n",
    "        \"Architecture decision records (ADRs)\",\n",
    "        \"Engineering blog posts\"\n",
    "    ],\n",
    "    \"best_practices\": [\n",
    "        \"Google's Engineering Practices\",\n",
    "        \"Microsoft's DevOps guides\", \n",
    "        \"Martin Fowler's architecture patterns\",\n",
    "        \"Clean Code principles\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Structured Learning Datasets** 🎓\n",
    "```python\n",
    "specialized_datasets = {\n",
    "    \"code_review_data\": \"Pull requests with comments and improvements\",\n",
    "    \"bug_fix_patterns\": \"Issue → Solution pairs from bug tracking\",\n",
    "    \"refactoring_examples\": \"Before/after code transformations\",\n",
    "    \"test_generation\": \"Code → comprehensive test suite pairs\",\n",
    "    \"documentation_pairs\": \"Code → technical documentation\",\n",
    "    \"architecture_decisions\": \"Requirements → architecture solutions\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 🧠 Model Architecture Transformation\n",
    "\n",
    "### **Current HRM Architecture Limitations**\n",
    "```python\n",
    "current_limitations = {\n",
    "    \"context_length\": \"Limited to 81-cell grids\",\n",
    "    \"output_space\": \"Fixed vocabulary (digits 1-9)\",\n",
    "    \"reasoning_depth\": \"Single-domain constraint satisfaction\",\n",
    "    \"memory\": \"No long-term project memory\",\n",
    "    \"multi_modal\": \"Text-only, no code structure awareness\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Required SE Model Architecture** 🏗️\n",
    "```python\n",
    "se_model_requirements = {\n",
    "    \"context_length\": \"128K+ tokens (full codebases)\",\n",
    "    \"output_space\": \"Multi-language code generation\",\n",
    "    \"reasoning_depth\": \"Multi-step architectural planning\",\n",
    "    \"memory\": \"Project-wide context and history\",\n",
    "    \"multi_modal\": \"Code, docs, diagrams, requirements\",\n",
    "    \"tool_integration\": \"IDE, testing, deployment tools\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Recommended Architecture Evolution**\n",
    "\n",
    "#### **Phase 1: Foundation Model** (3-6 months)\n",
    "- **Base**: Large transformer (7B-13B parameters)\n",
    "- **Training**: Code completion and basic SE tasks\n",
    "- **Evaluation**: HumanEval, MBPP benchmarks\n",
    "\n",
    "#### **Phase 2: Reasoning Enhancement** (6-12 months) \n",
    "- **Architecture**: Add reasoning modules (similar to current HRM)\n",
    "- **Training**: Multi-step software engineering tasks\n",
    "- **Evaluation**: Real project contributions\n",
    "\n",
    "#### **Phase 3: Specialization** (12+ months)\n",
    "- **Domain**: Your \"Assignments System of Work\"\n",
    "- **Training**: Custom dataset for your specific use case\n",
    "- **Integration**: Full workflow automation\n",
    "\n",
    "## 📊 Training Methodology\n",
    "\n",
    "### **Curriculum Learning Approach**\n",
    "```python\n",
    "training_phases = {\n",
    "    \"phase_1_basics\": {\n",
    "        \"duration\": \"2-4 weeks\",\n",
    "        \"focus\": \"Syntax, basic patterns, simple functions\",\n",
    "        \"datasets\": \"Code completion, syntax correction\"\n",
    "    },\n",
    "    \"phase_2_reasoning\": {\n",
    "        \"duration\": \"4-8 weeks\", \n",
    "        \"focus\": \"Multi-file projects, architecture decisions\",\n",
    "        \"datasets\": \"Repository analysis, design patterns\"\n",
    "    },\n",
    "    \"phase_3_engineering\": {\n",
    "        \"duration\": \"8-16 weeks\",\n",
    "        \"focus\": \"Full SDLC, testing, deployment\",\n",
    "        \"datasets\": \"End-to-end project development\"\n",
    "    },\n",
    "    \"phase_4_specialization\": {\n",
    "        \"duration\": \"Ongoing\",\n",
    "        \"focus\": \"Your specific domain and requirements\",\n",
    "        \"datasets\": \"Custom assignments system data\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Evaluation Metrics for SE Model**\n",
    "```python\n",
    "evaluation_framework = {\n",
    "    \"code_quality\": {\n",
    "        \"correctness\": \"Does the code run and solve the problem?\",\n",
    "        \"efficiency\": \"Is it performant and scalable?\",\n",
    "        \"maintainability\": \"Is it readable and well-structured?\",\n",
    "        \"security\": \"Does it follow security best practices?\"\n",
    "    },\n",
    "    \"engineering_practices\": {\n",
    "        \"testing\": \"Are comprehensive tests generated?\",\n",
    "        \"documentation\": \"Is code properly documented?\",\n",
    "        \"architecture\": \"Are sound architectural decisions made?\",\n",
    "        \"process\": \"Are SDLC best practices followed?\"\n",
    "    },\n",
    "    \"domain_expertise\": {\n",
    "        \"assignments_system\": \"Can it contribute to your specific project?\",\n",
    "        \"workflow_optimization\": \"Can it improve engineering processes?\",\n",
    "        \"knowledge_transfer\": \"Can it teach and mentor effectively?\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 🚀 Implementation Roadmap\n",
    "\n",
    "### **Immediate Next Steps** (This Month)\n",
    "1. **Complete current Sudoku training** (establish baseline capabilities)\n",
    "2. **Research existing code models** (CodeT5, CodeBERT, GitHub Copilot architecture)\n",
    "3. **Define specific SE requirements** for your Assignments System\n",
    "4. **Gather initial training datasets** (curate high-quality code repositories)\n",
    "\n",
    "### **Short Term** (Next 3 Months)\n",
    "1. **Architecture design** for SE-focused transformer\n",
    "2. **Data preprocessing pipeline** for code and documentation\n",
    "3. **Initial training experiments** on code completion tasks\n",
    "4. **Evaluation framework setup** for code quality assessment\n",
    "\n",
    "### **Medium Term** (6-12 Months)\n",
    "1. **Full-scale SE model training** with reasoning capabilities\n",
    "2. **Integration with development tools** (VS Code, Git, CI/CD)\n",
    "3. **Specialization training** on your domain-specific requirements\n",
    "4. **Human feedback integration** for continuous improvement\n",
    "\n",
    "## 🎯 Your \"Assignments System of Work\" Focus\n",
    "\n",
    "### **Domain-Specific Training Considerations**\n",
    "- **Workflow automation** patterns and practices\n",
    "- **Project management** methodologies and tools\n",
    "- **Team collaboration** frameworks and communication\n",
    "- **Quality assurance** and delivery processes\n",
    "- **Knowledge management** and documentation systems\n",
    "\n",
    "This is an incredibly ambitious project that could revolutionize how software engineering work is assigned, managed, and executed! 🚀\n",
    "\n",
    "Ready to discuss any specific aspect of this transformation plan?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
